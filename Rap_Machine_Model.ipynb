{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rap Machine Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev_VtcIWEHQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20485e96-9f9c-4a69-925d-ddd81d7e791b"
      },
      "source": [
        "# Installation des librairies nécessaires\n",
        "!pip install num2words\n",
        "!spacy download fr_core_news_md\n",
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n",
            "Collecting fr_core_news_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz (88.6MB)\n",
            "\u001b[K     |████████████████████████████████| 88.6MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: fr-core-news-md\n",
            "  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.2.5-cp36-none-any.whl size=90338489 sha256=4997b3d0d253f8ccdc9c2317a7a28e42c2c981340141c73e9f92582d59a499ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xt_280qe/wheels/c6/18/b6/f628642acc7872a53cf81269dd1c394d96da69564ccfac5425\n",
            "Successfully built fr-core-news-md\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aZTlgOj9c4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 500\n",
        "import re\n",
        "\n",
        "import tensorflow as tf \n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from num2words import num2words\n",
        "import spacy\n",
        "import fr_core_news_md\n",
        "nlp = fr_core_news_md.load()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GO5DZRV93k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('data.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPzzxbIX9_vQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4aa1a9ca-7a00-4814-e9c5-79639b42d04d"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LYRICS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lorsque débute la Première Guerre mondiale en 1914, \\nles pays européens exercent leur domination sur le monde, \\npar le biais de la colonisation.</td>\n",
              "      <td>1914-1918, 1ère guerre mondiale\\r\\nLes pays européens sont présents partout dans le monde\\r\\nEt ils exercent leur domination\\r\\nPar le biais de la colonisation\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>la Grande-Bretagne (l’empire britannique est le plus vaste) et la France, qui colonisent l’Afrique, l’Asie et l’Océanie. La Russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé.</td>\n",
              "      <td>La France et l’empire britannique sont en tête de liste\\r\\nEn Asie, en Océanie et en Afrique\\r\\nLa Russie ne possède pas de colonie\\r\\nMais c’est malgré tout un très grand pays\\r\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Si la fin du XIXe siècle est marquée par la suprématie européenne, elle se caractérise aussi par de multiples tensions et rivalités. L’attentat de Sarajevo le 28 juin 1914 est l’événement déclencheur d’une guerre d’abord européenne, puis mondiale.</td>\n",
              "      <td>Beaucoup de rivalité entre les pays\\r\\nEt montée du sentiment nationaliste\\r\\nLes différentes puissances nouent des alliances\\r\\nEt multiplient leurs dépenses d’armements\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dans ce contexte, les principales puissances européennes nouent rapidement des alliances : l’Allemagne et l’Autriche-Hongrie créent avec l’Italie la Triple Alliance, tandis que la France, la Russie et l’Angleterre se rapprochent dans une Triple Entente. En parallèle, chaque État multiplie ses dépenses d’armement. L’Allemagne est le pays qui investit le plus pour la fabrication d’armes.</td>\n",
              "      <td>L’Europe se divise en deux grands camps\\nLes Empires Centraux face à la Triple entente\\nAllemagne, Autriche-Hongrie et plus tard l’Italie\\nFace à la France, Le Royaume-Uni et La Russie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L’archiduc François Ferdinand était l’héritier du trône d’Autriche-Hongrie, rivale de la Serbie dans la région dite des Balkans. Il est assassiné à Sarajevo (en Bosnie-Herzégovine, annexée depuis 1908 par l’Autriche-Hongrie) le 28 juin 1914 par un nationaliste serbe.</td>\n",
              "      <td>Assassinat à Sarajevo en 1914\\r\\nDe l’héritier du trône d’Autriche-Hongrie, François Ferdinand\\r\\nIl se fait assassiner par un serbe nationaliste\\r\\nL’Autriche-Hongrie déclare la guerre à la Serbie\\r</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                   TEXT                                                                                                                                                                                                   LYRICS\n",
              "0                                                                                                                                                                                                                                                   Lorsque débute la Première Guerre mondiale en 1914, \\nles pays européens exercent leur domination sur le monde, \\npar le biais de la colonisation.                                         1914-1918, 1ère guerre mondiale\\r\\nLes pays européens sont présents partout dans le monde\\r\\nEt ils exercent leur domination\\r\\nPar le biais de la colonisation\\r\n",
              "1                                                                                                                                                                                      la Grande-Bretagne (l’empire britannique est le plus vaste) et la France, qui colonisent l’Afrique, l’Asie et l’Océanie. La Russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé.                     La France et l’empire britannique sont en tête de liste\\r\\nEn Asie, en Océanie et en Afrique\\r\\nLa Russie ne possède pas de colonie\\r\\nMais c’est malgré tout un très grand pays\\r\\n\n",
              "2                                                                                                                                               Si la fin du XIXe siècle est marquée par la suprématie européenne, elle se caractérise aussi par de multiples tensions et rivalités. L’attentat de Sarajevo le 28 juin 1914 est l’événement déclencheur d’une guerre d’abord européenne, puis mondiale.                             Beaucoup de rivalité entre les pays\\r\\nEt montée du sentiment nationaliste\\r\\nLes différentes puissances nouent des alliances\\r\\nEt multiplient leurs dépenses d’armements\\r\n",
              "3  Dans ce contexte, les principales puissances européennes nouent rapidement des alliances : l’Allemagne et l’Autriche-Hongrie créent avec l’Italie la Triple Alliance, tandis que la France, la Russie et l’Angleterre se rapprochent dans une Triple Entente. En parallèle, chaque État multiplie ses dépenses d’armement. L’Allemagne est le pays qui investit le plus pour la fabrication d’armes.                 L’Europe se divise en deux grands camps\\nLes Empires Centraux face à la Triple entente\\nAllemagne, Autriche-Hongrie et plus tard l’Italie\\nFace à la France, Le Royaume-Uni et La Russie\n",
              "4                                                                                                                          L’archiduc François Ferdinand était l’héritier du trône d’Autriche-Hongrie, rivale de la Serbie dans la région dite des Balkans. Il est assassiné à Sarajevo (en Bosnie-Herzégovine, annexée depuis 1908 par l’Autriche-Hongrie) le 28 juin 1914 par un nationaliste serbe.   Assassinat à Sarajevo en 1914\\r\\nDe l’héritier du trône d’Autriche-Hongrie, François Ferdinand\\r\\nIl se fait assassiner par un serbe nationaliste\\r\\nL’Autriche-Hongrie déclare la guerre à la Serbie\\r"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAIdBvsi-BmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70cc3216-ad30-4ea9-d643-7485e5cbc794"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(611, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VsyasSPjNHf",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpyVnYW_MobJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformation des chiffres en mots\n",
        "def numbers_to_words(string):\n",
        "  string = string.split()\n",
        "  string = [ num2words(el, lang='fr') if el.isdigit() else el for el in string ]\n",
        "  string = ' '.join(string)\n",
        "  return string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTg9iou4fe1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Suppression des chiffres inutils\n",
        "def remove_single_number(string): \n",
        "    pattern = '[0-9]'\n",
        "    string = string.split()\n",
        "    string = [re.sub(pattern, '', i) for i in string] \n",
        "    string = ' '.join(string)\n",
        "    return string"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRZ6kJIpD99L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Suppression des caractères spéciaux\n",
        "def remove_special_characters(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace('\\r\\n', ' <lb> ') # préparation d'encodage des retours à la ligne\n",
        "  text = re.sub(\"[|\\^&+\\-%*/=!>:]\\'\", '', text)\n",
        "  text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "  text = text.replace(',', '')\n",
        "  text = text.replace('.', '')\n",
        "  text = text.replace('-', ' ')\n",
        "  text = \" \".join(text.split())\n",
        "  return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "634x1SB8j9LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction de preprocessing générale\n",
        "def preprocess_string(string):\n",
        "  string = remove_special_characters(string)\n",
        "  string = numbers_to_words(string)\n",
        "  string = remove_single_number(string)\n",
        "  #string = remove_stop_words(string)\n",
        "  return string"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0hGyqZn_DkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['TEXT'] = data['TEXT'].apply(preprocess_string)\n",
        "data['LYRICS'] = data['LYRICS'].apply(preprocess_string)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCvRMTe3KnJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5d2e73f3-8b98-44f3-df65-c74c302def93"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LYRICS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lorsque débute la première guerre mondiale en mille neuf cent quatorze les pays européens exercent leur domination sur le monde par le biais de la colonisation</td>\n",
              "      <td>mille neuf cent quatorze mille neuf cent dix-huit ère guerre mondiale &lt;lb&gt; les pays européens sont présents partout dans le monde &lt;lb&gt; et ils exercent leur domination &lt;lb&gt; par le biais de la colonisation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>la grande bretagne et la france qui colonisent l’afrique l’asie et l’océanie la russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé</td>\n",
              "      <td>la france et l’empire britannique sont en tête de liste &lt;lb&gt; en asie en océanie et en afrique &lt;lb&gt; la russie ne possède pas de colonie &lt;lb&gt; mais c’est malgré tout un très grand pays &lt;lb&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>si la fin du xixe siècle est marquée par la suprématie européenne elle se caractérise aussi par de multiples tensions et rivalités l’attentat de sarajevo le vingt-huit juin mille neuf cent quatorze est l’événement déclencheur d’une guerre d’abord européenne puis mondiale</td>\n",
              "      <td>beaucoup de rivalité entre les pays &lt;lb&gt; et montée du sentiment nationaliste &lt;lb&gt; les différentes puissances nouent des alliances &lt;lb&gt; et multiplient leurs dépenses d’armements</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dans ce contexte les principales puissances européennes nouent rapidement des alliances : l’allemagne et l’autriche hongrie créent avec l’italie la triple alliance tandis que la france la russie et l’angleterre se rapprochent dans une triple entente en parallèle chaque état multiplie ses dépenses d’armement l’allemagne est le pays qui investit le plus pour la fabrication d’armes</td>\n",
              "      <td>l’europe se divise en deux grands camps les empires centraux face à la triple entente allemagne autriche hongrie et plus tard l’italie face à la france le royaume uni et la russie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>l’archiduc françois ferdinand était l’héritier du trône d’autriche hongrie rivale de la serbie dans la région dite des balkans il est assassiné à sarajevo le vingt-huit juin mille neuf cent quatorze par un nationaliste serbe</td>\n",
              "      <td>assassinat à sarajevo en mille neuf cent quatorze &lt;lb&gt; de l’héritier du trône d’autriche hongrie françois ferdinand &lt;lb&gt; il se fait assassiner par un serbe nationaliste &lt;lb&gt; l’autriche hongrie déclare la guerre à la serbie</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                            TEXT                                                                                                                                                                                                                          LYRICS\n",
              "0                                                                                                                                                                                                                                lorsque débute la première guerre mondiale en mille neuf cent quatorze les pays européens exercent leur domination sur le monde par le biais de la colonisation                     mille neuf cent quatorze mille neuf cent dix-huit ère guerre mondiale <lb> les pays européens sont présents partout dans le monde <lb> et ils exercent leur domination <lb> par le biais de la colonisation\n",
              "1                                                                                                                                                                                                                            la grande bretagne et la france qui colonisent l’afrique l’asie et l’océanie la russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé                                      la france et l’empire britannique sont en tête de liste <lb> en asie en océanie et en afrique <lb> la russie ne possède pas de colonie <lb> mais c’est malgré tout un très grand pays <lb>\n",
              "2                                                                                                                si la fin du xixe siècle est marquée par la suprématie européenne elle se caractérise aussi par de multiples tensions et rivalités l’attentat de sarajevo le vingt-huit juin mille neuf cent quatorze est l’événement déclencheur d’une guerre d’abord européenne puis mondiale                                                beaucoup de rivalité entre les pays <lb> et montée du sentiment nationaliste <lb> les différentes puissances nouent des alliances <lb> et multiplient leurs dépenses d’armements\n",
              "3  dans ce contexte les principales puissances européennes nouent rapidement des alliances : l’allemagne et l’autriche hongrie créent avec l’italie la triple alliance tandis que la france la russie et l’angleterre se rapprochent dans une triple entente en parallèle chaque état multiplie ses dépenses d’armement l’allemagne est le pays qui investit le plus pour la fabrication d’armes                                             l’europe se divise en deux grands camps les empires centraux face à la triple entente allemagne autriche hongrie et plus tard l’italie face à la france le royaume uni et la russie\n",
              "4                                                                                                                                                               l’archiduc françois ferdinand était l’héritier du trône d’autriche hongrie rivale de la serbie dans la région dite des balkans il est assassiné à sarajevo le vingt-huit juin mille neuf cent quatorze par un nationaliste serbe  assassinat à sarajevo en mille neuf cent quatorze <lb> de l’héritier du trône d’autriche hongrie françois ferdinand <lb> il se fait assassiner par un serbe nationaliste <lb> l’autriche hongrie déclare la guerre à la serbie"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTB6QM36RIJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ajout des balises <start> et <end> sur la target avant l'encodage\n",
        "\n",
        "def begin_end_sentence(sentence):\n",
        "  sentence = \"<start> \"+ sentence + \" <end>\"\n",
        "  return sentence\n",
        "\n",
        "data['LYRICS'] = data['LYRICS'].apply(lambda x: begin_end_sentence(x))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9jQ6JUHNELH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ajout des balises spéciales dans le corpus\n",
        "\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "start_case = [{ORTH:\"<start>\"}]\n",
        "end_case = [{ORTH: \"<end>\"}]\n",
        "line_break = [{ORTH: \"<lb>\"}]\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"<start>\", start_case)\n",
        "nlp.tokenizer.add_special_case(\"<end>\", end_case)\n",
        "nlp.tokenizer.add_special_case(\"<lb>\", line_break)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1f_ZbUXHZd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "161a168d-e472-49f8-bc98-1b6a6bf51701"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LYRICS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lorsque débute la première guerre mondiale en mille neuf cent quatorze les pays européens exercent leur domination sur le monde par le biais de la colonisation</td>\n",
              "      <td>&lt;start&gt; mille neuf cent quatorze mille neuf cent dix-huit ère guerre mondiale &lt;lb&gt; les pays européens sont présents partout dans le monde &lt;lb&gt; et ils exercent leur domination &lt;lb&gt; par le biais de la colonisation &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>la grande bretagne et la france qui colonisent l’afrique l’asie et l’océanie la russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé</td>\n",
              "      <td>&lt;start&gt; la france et l’empire britannique sont en tête de liste &lt;lb&gt; en asie en océanie et en afrique &lt;lb&gt; la russie ne possède pas de colonie &lt;lb&gt; mais c’est malgré tout un très grand pays &lt;lb&gt; &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>si la fin du xixe siècle est marquée par la suprématie européenne elle se caractérise aussi par de multiples tensions et rivalités l’attentat de sarajevo le vingt-huit juin mille neuf cent quatorze est l’événement déclencheur d’une guerre d’abord européenne puis mondiale</td>\n",
              "      <td>&lt;start&gt; beaucoup de rivalité entre les pays &lt;lb&gt; et montée du sentiment nationaliste &lt;lb&gt; les différentes puissances nouent des alliances &lt;lb&gt; et multiplient leurs dépenses d’armements &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dans ce contexte les principales puissances européennes nouent rapidement des alliances : l’allemagne et l’autriche hongrie créent avec l’italie la triple alliance tandis que la france la russie et l’angleterre se rapprochent dans une triple entente en parallèle chaque état multiplie ses dépenses d’armement l’allemagne est le pays qui investit le plus pour la fabrication d’armes</td>\n",
              "      <td>&lt;start&gt; l’europe se divise en deux grands camps les empires centraux face à la triple entente allemagne autriche hongrie et plus tard l’italie face à la france le royaume uni et la russie &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>l’archiduc françois ferdinand était l’héritier du trône d’autriche hongrie rivale de la serbie dans la région dite des balkans il est assassiné à sarajevo le vingt-huit juin mille neuf cent quatorze par un nationaliste serbe</td>\n",
              "      <td>&lt;start&gt; assassinat à sarajevo en mille neuf cent quatorze &lt;lb&gt; de l’héritier du trône d’autriche hongrie françois ferdinand &lt;lb&gt; il se fait assassiner par un serbe nationaliste &lt;lb&gt; l’autriche hongrie déclare la guerre à la serbie &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                            TEXT                                                                                                                                                                                                                                        LYRICS\n",
              "0                                                                                                                                                                                                                                lorsque débute la première guerre mondiale en mille neuf cent quatorze les pays européens exercent leur domination sur le monde par le biais de la colonisation                     <start> mille neuf cent quatorze mille neuf cent dix-huit ère guerre mondiale <lb> les pays européens sont présents partout dans le monde <lb> et ils exercent leur domination <lb> par le biais de la colonisation <end>\n",
              "1                                                                                                                                                                                                                            la grande bretagne et la france qui colonisent l’afrique l’asie et l’océanie la russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé                                      <start> la france et l’empire britannique sont en tête de liste <lb> en asie en océanie et en afrique <lb> la russie ne possède pas de colonie <lb> mais c’est malgré tout un très grand pays <lb> <end>\n",
              "2                                                                                                                si la fin du xixe siècle est marquée par la suprématie européenne elle se caractérise aussi par de multiples tensions et rivalités l’attentat de sarajevo le vingt-huit juin mille neuf cent quatorze est l’événement déclencheur d’une guerre d’abord européenne puis mondiale                                                <start> beaucoup de rivalité entre les pays <lb> et montée du sentiment nationaliste <lb> les différentes puissances nouent des alliances <lb> et multiplient leurs dépenses d’armements <end>\n",
              "3  dans ce contexte les principales puissances européennes nouent rapidement des alliances : l’allemagne et l’autriche hongrie créent avec l’italie la triple alliance tandis que la france la russie et l’angleterre se rapprochent dans une triple entente en parallèle chaque état multiplie ses dépenses d’armement l’allemagne est le pays qui investit le plus pour la fabrication d’armes                                             <start> l’europe se divise en deux grands camps les empires centraux face à la triple entente allemagne autriche hongrie et plus tard l’italie face à la france le royaume uni et la russie <end>\n",
              "4                                                                                                                                                               l’archiduc françois ferdinand était l’héritier du trône d’autriche hongrie rivale de la serbie dans la région dite des balkans il est assassiné à sarajevo le vingt-huit juin mille neuf cent quatorze par un nationaliste serbe  <start> assassinat à sarajevo en mille neuf cent quatorze <lb> de l’héritier du trône d’autriche hongrie françois ferdinand <lb> il se fait assassiner par un serbe nationaliste <lb> l’autriche hongrie déclare la guerre à la serbie <end>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTYH6GMJNu0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création du corpus\n",
        "\n",
        "array_text = data['TEXT'].values \n",
        "array_lyrics = data['LYRICS'].values\n",
        "array_lyrics = ' '.join(array_lyrics)\n",
        "array_text = ' '.join(array_text)\n",
        "all_text = array_lyrics + array_text "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H33pP2GmTZ3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.max_length = len(all_text)\n",
        "\n",
        "doc = nlp(all_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zpaAJmgM3gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5be5da4-a435-479f-dadd-53e6233b439f"
      },
      "source": [
        "# Création du vocabulaire\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "vocabulary_set = set(tokens)\n",
        "vocab_size = len(vocabulary_set)\n",
        "print(vocab_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIUozHtx-7Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création du dictionnaire d'encodage\n",
        " \n",
        "all_tokens = {}\n",
        "for i,token in enumerate(vocabulary_set):\n",
        "  all_tokens[token] = i+1 "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfFK-WQ2OxV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "data['TEXT_TOKENS'] = data['TEXT'].map(lambda x : nlp.tokenizer(x))\n",
        "data['LYRICS_TOKENS'] = data['LYRICS'].map(lambda x : nlp.tokenizer(x))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4gx3KkkMWe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création des indices\n",
        "\n",
        "def tokens_to_index(tokens):\n",
        "  indices = []\n",
        "  for token in tokens:\n",
        "    indices.append(all_tokens[token.text])\n",
        "  \n",
        "  return indices"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJqubv4q5kV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encodage\n",
        "\n",
        "data['TEXT_ENCODED'] = data['TEXT_TOKENS'].map(lambda x: tokens_to_index(x))\n",
        "data['LYRICS_ENCODED'] = data['LYRICS_TOKENS'].map(lambda x: tokens_to_index(x))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC9uN9aH6Miq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "917d5a08-2ac1-4bb1-ed34-100e1e550f71"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LYRICS</th>\n",
              "      <th>TEXT_TOKENS</th>\n",
              "      <th>LYRICS_TOKENS</th>\n",
              "      <th>TEXT_ENCODED</th>\n",
              "      <th>LYRICS_ENCODED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lorsque débute la première guerre mondiale en mille neuf cent quatorze les pays européens exercent leur domination sur le monde par le biais de la colonisation</td>\n",
              "      <td>&lt;start&gt; mille neuf cent quatorze mille neuf cent dix-huit ère guerre mondiale &lt;lb&gt; les pays européens sont présents partout dans le monde &lt;lb&gt; et ils exercent leur domination &lt;lb&gt; par le biais de la colonisation &lt;end&gt;</td>\n",
              "      <td>(lorsque, débute, la, première, guerre, mondiale, en, mille, neuf, cent, quatorze, les, pays, européens, exercent, leur, domination, sur, le, monde, par, le, biais, de, la, colonisation)</td>\n",
              "      <td>(&lt;start&gt;, mille, neuf, cent, quatorze, mille, neuf, cent, dix-huit, ère, guerre, mondiale, &lt;lb&gt;, les, pays, européens, sont, présents, partout, dans, le, monde, &lt;lb&gt;, et, ils, exercent, leur, domination, &lt;lb&gt;, par, le, biais, de, la, colonisation, &lt;end&gt;)</td>\n",
              "      <td>[2126, 3188, 1807, 2874, 4058, 46, 1617, 462, 2667, 3511, 1172, 2473, 2925, 1933, 1359, 2986, 1709, 3561, 892, 2974, 3353, 892, 597, 3940, 1807, 4206]</td>\n",
              "      <td>[1782, 462, 2667, 3511, 1172, 462, 2667, 3511, 2372, 1553, 4058, 46, 787, 2473, 2925, 1933, 2261, 2416, 4316, 1780, 892, 2974, 787, 3389, 3294, 1359, 2986, 1709, 787, 3353, 892, 597, 3940, 1807, 4206, 2872]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>la grande bretagne et la france qui colonisent l’afrique l’asie et l’océanie la russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé</td>\n",
              "      <td>&lt;start&gt; la france et l’empire britannique sont en tête de liste &lt;lb&gt; en asie en océanie et en afrique &lt;lb&gt; la russie ne possède pas de colonie &lt;lb&gt; mais c’est malgré tout un très grand pays &lt;lb&gt; &lt;end&gt;</td>\n",
              "      <td>(la, grande, bretagne, et, la, france, qui, colonisent, l’, afrique, l’, asie, et, l’, océanie, la, russie, n’, a, pas, de, colonies, mais, elle, profite, d’, un, territoire, immense, et, très, peuplé)</td>\n",
              "      <td>(&lt;start&gt;, la, france, et, l’, empire, britannique, sont, en, tête, de, liste, &lt;lb&gt;, en, asie, en, océanie, et, en, afrique, &lt;lb&gt;, la, russie, ne, possède, pas, de, colonie, &lt;lb&gt;, mais, c’, est, malgré, tout, un, très, grand, pays, &lt;lb&gt;, &lt;end&gt;)</td>\n",
              "      <td>[1807, 1406, 2744, 3389, 1807, 3780, 2060, 4213, 3581, 567, 3581, 1576, 3389, 3581, 1011, 1807, 2366, 582, 3216, 1217, 3940, 3595, 2773, 870, 3766, 3734, 4356, 1505, 1021, 3389, 3462, 815]</td>\n",
              "      <td>[1782, 1807, 3780, 3389, 3581, 2306, 4073, 2261, 1617, 1861, 3940, 655, 787, 1617, 1576, 1617, 1011, 3389, 1617, 567, 787, 1807, 2366, 2769, 2421, 1217, 3940, 3500, 787, 2773, 82, 43, 3273, 4397, 4356, 3462, 428, 2925, 787, 2872]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>si la fin du xixe siècle est marquée par la suprématie européenne elle se caractérise aussi par de multiples tensions et rivalités l’attentat de sarajevo le vingt-huit juin mille neuf cent quatorze est l’événement déclencheur d’une guerre d’abord européenne puis mondiale</td>\n",
              "      <td>&lt;start&gt; beaucoup de rivalité entre les pays &lt;lb&gt; et montée du sentiment nationaliste &lt;lb&gt; les différentes puissances nouent des alliances &lt;lb&gt; et multiplient leurs dépenses d’armements &lt;end&gt;</td>\n",
              "      <td>(si, la, fin, du, xixe, siècle, est, marquée, par, la, suprématie, européenne, elle, se, caractérise, aussi, par, de, multiples, tensions, et, rivalités, l’, attentat, de, sarajevo, le, vingt-huit, juin, mille, neuf, cent, quatorze, est, l’, événement, déclencheur, d’, une, guerre, d’, abord, européenne, puis, mondiale)</td>\n",
              "      <td>(&lt;start&gt;, beaucoup, de, rivalité, entre, les, pays, &lt;lb&gt;, et, montée, du, sentiment, nationaliste, &lt;lb&gt;, les, différentes, puissances, nouent, des, alliances, &lt;lb&gt;, et, multiplient, leurs, dépenses, d’, armements, &lt;end&gt;)</td>\n",
              "      <td>[1616, 1807, 2341, 822, 1644, 3447, 43, 404, 3353, 1807, 561, 2073, 870, 1960, 3194, 3183, 3353, 3940, 2690, 827, 3389, 4312, 3581, 4237, 3940, 3618, 892, 1898, 4366, 462, 2667, 3511, 1172, 43, 3581, 3714, 1516, 3734, 2944, 4058, 3734, 4319, 2073, 3882, 46]</td>\n",
              "      <td>[1782, 1495, 3940, 1583, 194, 2473, 2925, 787, 3389, 379, 822, 3435, 794, 787, 2473, 1218, 4033, 1364, 3792, 3840, 787, 3389, 203, 1324, 1086, 3734, 2468, 2872]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dans ce contexte les principales puissances européennes nouent rapidement des alliances : l’allemagne et l’autriche hongrie créent avec l’italie la triple alliance tandis que la france la russie et l’angleterre se rapprochent dans une triple entente en parallèle chaque état multiplie ses dépenses d’armement l’allemagne est le pays qui investit le plus pour la fabrication d’armes</td>\n",
              "      <td>&lt;start&gt; l’europe se divise en deux grands camps les empires centraux face à la triple entente allemagne autriche hongrie et plus tard l’italie face à la france le royaume uni et la russie &lt;end&gt;</td>\n",
              "      <td>(dans, ce, contexte, les, principales, puissances, européennes, nouent, rapidement, des, alliances, :, l’, allemagne, et, l’, autriche, hongrie, créent, avec, l’, italie, la, triple, alliance, tandis, que, la, france, la, russie, et, l’, angleterre, se, rapprochent, dans, une, triple, entente, en, parallèle, chaque, état, multiplie, ses, dépenses, d’, armement, l’, allemagne, est, le, pays, qui, investit, le, plus, pour, la, fabrication, d’, armes)</td>\n",
              "      <td>(&lt;start&gt;, l’, europe, se, divise, en, deux, grands, camps, les, empires, centraux, face, à, la, triple, entente, allemagne, autriche, hongrie, et, plus, tard, l’, italie, face, à, la, france, le, royaume, uni, et, la, russie, &lt;end&gt;)</td>\n",
              "      <td>[1780, 709, 1585, 2473, 1959, 4033, 317, 1364, 3200, 3792, 3840, 2357, 3581, 4352, 3389, 3581, 3198, 3105, 469, 2435, 3581, 259, 1807, 3415, 4081, 7, 145, 1807, 3780, 1807, 2366, 3389, 3581, 3773, 1960, 3634, 1780, 2944, 3415, 3312, 1617, 1605, 3082, 3578, 2978, 4173, 1086, 3734, 297, 3581, 4352, 43, 892, 2925, 2060, 3163, 892, 2065, 458, 1807, 1743, 3734, 2746]</td>\n",
              "      <td>[1782, 3581, 174, 1960, 440, 1617, 2889, 2467, 722, 2473, 1642, 1119, 81, 3798, 1807, 3415, 3312, 4352, 3198, 3105, 3389, 2065, 4304, 3581, 259, 81, 3798, 1807, 3780, 892, 2630, 2495, 3389, 1807, 2366, 2872]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>l’archiduc françois ferdinand était l’héritier du trône d’autriche hongrie rivale de la serbie dans la région dite des balkans il est assassiné à sarajevo le vingt-huit juin mille neuf cent quatorze par un nationaliste serbe</td>\n",
              "      <td>&lt;start&gt; assassinat à sarajevo en mille neuf cent quatorze &lt;lb&gt; de l’héritier du trône d’autriche hongrie françois ferdinand &lt;lb&gt; il se fait assassiner par un serbe nationaliste &lt;lb&gt; l’autriche hongrie déclare la guerre à la serbie &lt;end&gt;</td>\n",
              "      <td>(l’, archiduc, françois, ferdinand, était, l’, héritier, du, trône, d’, autriche, hongrie, rivale, de, la, serbie, dans, la, région, dite, des, balkans, il, est, assassiné, à, sarajevo, le, vingt-huit, juin, mille, neuf, cent, quatorze, par, un, nationaliste, serbe)</td>\n",
              "      <td>(&lt;start&gt;, assassinat, à, sarajevo, en, mille, neuf, cent, quatorze, &lt;lb&gt;, de, l’, héritier, du, trône, d’, autriche, hongrie, françois, ferdinand, &lt;lb&gt;, il, se, fait, assassiner, par, un, serbe, nationaliste, &lt;lb&gt;, l’, autriche, hongrie, déclare, la, guerre, à, la, serbie, &lt;end&gt;)</td>\n",
              "      <td>[3581, 3695, 2627, 2033, 1964, 3581, 4162, 822, 2529, 3734, 3198, 3105, 1181, 3940, 1807, 2794, 1780, 1807, 4159, 3292, 3792, 1114, 2486, 43, 1493, 3798, 3618, 892, 1898, 4366, 462, 2667, 3511, 1172, 3353, 4356, 794, 37]</td>\n",
              "      <td>[1782, 4225, 3798, 3618, 1617, 462, 2667, 3511, 1172, 787, 3940, 3581, 4162, 822, 2529, 3734, 3198, 3105, 2627, 2033, 787, 2486, 1960, 821, 3977, 3353, 4356, 37, 794, 787, 3581, 3198, 3105, 3038, 1807, 4058, 3798, 1807, 2794, 2872]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                            TEXT  ...                                                                                                                                                                                                                           LYRICS_ENCODED\n",
              "0                                                                                                                                                                                                                                lorsque débute la première guerre mondiale en mille neuf cent quatorze les pays européens exercent leur domination sur le monde par le biais de la colonisation  ...                           [1782, 462, 2667, 3511, 1172, 462, 2667, 3511, 2372, 1553, 4058, 46, 787, 2473, 2925, 1933, 2261, 2416, 4316, 1780, 892, 2974, 787, 3389, 3294, 1359, 2986, 1709, 787, 3353, 892, 597, 3940, 1807, 4206, 2872]\n",
              "1                                                                                                                                                                                                                            la grande bretagne et la france qui colonisent l’afrique l’asie et l’océanie la russie n’a pas de colonies mais elle profite d’un territoire immense et très peuplé  ...    [1782, 1807, 3780, 3389, 3581, 2306, 4073, 2261, 1617, 1861, 3940, 655, 787, 1617, 1576, 1617, 1011, 3389, 1617, 567, 787, 1807, 2366, 2769, 2421, 1217, 3940, 3500, 787, 2773, 82, 43, 3273, 4397, 4356, 3462, 428, 2925, 787, 2872]\n",
              "2                                                                                                                si la fin du xixe siècle est marquée par la suprématie européenne elle se caractérise aussi par de multiples tensions et rivalités l’attentat de sarajevo le vingt-huit juin mille neuf cent quatorze est l’événement déclencheur d’une guerre d’abord européenne puis mondiale  ...                                                                         [1782, 1495, 3940, 1583, 194, 2473, 2925, 787, 3389, 379, 822, 3435, 794, 787, 2473, 1218, 4033, 1364, 3792, 3840, 787, 3389, 203, 1324, 1086, 3734, 2468, 2872]\n",
              "3  dans ce contexte les principales puissances européennes nouent rapidement des alliances : l’allemagne et l’autriche hongrie créent avec l’italie la triple alliance tandis que la france la russie et l’angleterre se rapprochent dans une triple entente en parallèle chaque état multiplie ses dépenses d’armement l’allemagne est le pays qui investit le plus pour la fabrication d’armes  ...                          [1782, 3581, 174, 1960, 440, 1617, 2889, 2467, 722, 2473, 1642, 1119, 81, 3798, 1807, 3415, 3312, 4352, 3198, 3105, 3389, 2065, 4304, 3581, 259, 81, 3798, 1807, 3780, 892, 2630, 2495, 3389, 1807, 2366, 2872]\n",
              "4                                                                                                                                                               l’archiduc françois ferdinand était l’héritier du trône d’autriche hongrie rivale de la serbie dans la région dite des balkans il est assassiné à sarajevo le vingt-huit juin mille neuf cent quatorze par un nationaliste serbe  ...  [1782, 4225, 3798, 3618, 1617, 462, 2667, 3511, 1172, 787, 3940, 3581, 4162, 822, 2529, 3734, 3198, 3105, 2627, 2033, 787, 2486, 1960, 821, 3977, 3353, 4356, 37, 794, 787, 3581, 3198, 3105, 3038, 1807, 4058, 3798, 1807, 2794, 2872]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COwXTEBc8DY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(lines):\n",
        "  return max(len(line) for line in lines)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul9D_l-B8SZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_max_len = max_len(data['TEXT_ENCODED'].to_list())\n",
        "lyrics_max_len = max_len(data['LYRICS_ENCODED'].to_list())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHVUv2bN8oM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding et création du dataset tensorflow\n",
        "\n",
        "padded_text_indices = tf.keras.preprocessing.sequence.pad_sequences(data[\"TEXT_ENCODED\"], maxlen=text_max_len, padding=\"post\")\n",
        "padded_lyrics_indices = tf.keras.preprocessing.sequence.pad_sequences(data[\"LYRICS_ENCODED\"], maxlen=lyrics_max_len, padding=\"post\")\n",
        "\n",
        "tf_ds = tf.data.Dataset.from_tensor_slices((padded_text_indices, padded_lyrics_indices))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr-Qh8T1IcYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Création du train_test et batch\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "TAKE_SIZE = int(0.9*len(data)/BATCH_SIZE)\n",
        "BUFFER_SIZE = TAKE_SIZE * BATCH_SIZE\n",
        "steps_per_epoch = TAKE_SIZE\n",
        "embedding_dim = 128\n",
        "units = 512\n",
        "\n",
        "tf_ds = tf_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "train_data = tf_ds.take(TAKE_SIZE)\n",
        "test_data = tf_ds.skip(TAKE_SIZE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtwFIAkTDO1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5bc39c94-39e6-4c90-b72f-26cb7fc5eb63"
      },
      "source": [
        "input_text, output_text = next(iter(train_data))\n",
        "print(input_text.numpy().shape)\n",
        "print(output_text.numpy().shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 84)\n",
            "(16, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6_JEwUeAtzN",
        "colab_type": "text"
      },
      "source": [
        "### MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4kBkRGrJJjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3pO7QBPJOMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f03c5735-d0e2-491f-b950-be30cb581dfd"
      },
      "source": [
        "encoder = Encoder(vocab_size +1, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Echantillon d'output\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(input_text, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (16, 84, 512)\n",
            "Encoder Hidden state shape: (batch size, units) (16, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCu3uxyqJiXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # Calcul du score \"d'attention\"\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # On obtient 1 sur le dernier axe car on applique le score à self.V\n",
        "    # La shape du tenseur avant que l'on applique self.V est (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gRKzCKWJiHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "27bd029e-769d-4bd2-cbad-1efd6ec2673f"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (16, 512)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (16, 84, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AvmwofJnyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Utilisé pour attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape après embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape après concaténation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # Passage du vecteur concaténé à la couche GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpStT3PBJwOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97cb8b6c-28dd-4f73-f9fb-d9f03f0ba4f0"
      },
      "source": [
        "decoder = Decoder(vocab_size + 1, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (16, 4408)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWR-1nZkJz7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9JrX6sCJ7Wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR6QiwYmJ_rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([all_tokens[\"<start>\"]] * BATCH_SIZE, 1)\n",
        "\n",
        "    \n",
        "    for t in range(1, targ.shape[1]):\n",
        "      \n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      \n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gshrj8slKNJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "391b2539-95d4-4d8b-f950-2ffc901f260f"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 300\n",
        "steps_per_epoch = TAKE_SIZE\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_data.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  \n",
        "  # Enregistrement (checkpoint) du modèle toutes les 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.2663\n",
            "Epoch 1 Batch 10 Loss 4.4480\n",
            "Epoch 1 Batch 20 Loss 4.7850\n",
            "Epoch 1 Batch 30 Loss 4.2706\n",
            "Epoch 1 Loss 4.7206\n",
            "Time taken for 1 epoch 49.808661460876465 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.0876\n",
            "Epoch 2 Batch 10 Loss 4.1465\n",
            "Epoch 2 Batch 20 Loss 4.2439\n",
            "Epoch 2 Batch 30 Loss 4.3527\n",
            "Epoch 2 Loss 4.2961\n",
            "Time taken for 1 epoch 4.338777542114258 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.1003\n",
            "Epoch 3 Batch 10 Loss 3.9497\n",
            "Epoch 3 Batch 20 Loss 4.1334\n",
            "Epoch 3 Batch 30 Loss 4.0341\n",
            "Epoch 3 Loss 4.1859\n",
            "Time taken for 1 epoch 4.143188953399658 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.1805\n",
            "Epoch 4 Batch 10 Loss 4.2261\n",
            "Epoch 4 Batch 20 Loss 4.0697\n",
            "Epoch 4 Batch 30 Loss 4.4413\n",
            "Epoch 4 Loss 4.1081\n",
            "Time taken for 1 epoch 4.404816627502441 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.9075\n",
            "Epoch 5 Batch 10 Loss 4.0593\n",
            "Epoch 5 Batch 20 Loss 4.2743\n",
            "Epoch 5 Batch 30 Loss 3.9038\n",
            "Epoch 5 Loss 4.0805\n",
            "Time taken for 1 epoch 4.221145391464233 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.8302\n",
            "Epoch 6 Batch 10 Loss 4.1971\n",
            "Epoch 6 Batch 20 Loss 3.7577\n",
            "Epoch 6 Batch 30 Loss 3.7784\n",
            "Epoch 6 Loss 4.0134\n",
            "Time taken for 1 epoch 4.3281402587890625 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 3.9148\n",
            "Epoch 7 Batch 10 Loss 4.0217\n",
            "Epoch 7 Batch 20 Loss 3.9590\n",
            "Epoch 7 Batch 30 Loss 4.0059\n",
            "Epoch 7 Loss 3.9708\n",
            "Time taken for 1 epoch 4.200329542160034 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 3.7362\n",
            "Epoch 8 Batch 10 Loss 3.5466\n",
            "Epoch 8 Batch 20 Loss 3.8452\n",
            "Epoch 8 Batch 30 Loss 4.1790\n",
            "Epoch 8 Loss 3.8820\n",
            "Time taken for 1 epoch 4.368613243103027 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.6219\n",
            "Epoch 9 Batch 10 Loss 3.5326\n",
            "Epoch 9 Batch 20 Loss 3.6029\n",
            "Epoch 9 Batch 30 Loss 3.6557\n",
            "Epoch 9 Loss 3.7774\n",
            "Time taken for 1 epoch 4.19390869140625 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.4234\n",
            "Epoch 10 Batch 10 Loss 3.6187\n",
            "Epoch 10 Batch 20 Loss 3.7272\n",
            "Epoch 10 Batch 30 Loss 3.5606\n",
            "Epoch 10 Loss 3.7146\n",
            "Time taken for 1 epoch 4.268614768981934 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.5648\n",
            "Epoch 11 Batch 10 Loss 3.5160\n",
            "Epoch 11 Batch 20 Loss 3.6531\n",
            "Epoch 11 Batch 30 Loss 3.8023\n",
            "Epoch 11 Loss 3.5802\n",
            "Time taken for 1 epoch 4.180800676345825 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.6268\n",
            "Epoch 12 Batch 10 Loss 3.4873\n",
            "Epoch 12 Batch 20 Loss 3.3509\n",
            "Epoch 12 Batch 30 Loss 3.5638\n",
            "Epoch 12 Loss 3.4664\n",
            "Time taken for 1 epoch 4.363612651824951 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.3773\n",
            "Epoch 13 Batch 10 Loss 3.3636\n",
            "Epoch 13 Batch 20 Loss 3.3886\n",
            "Epoch 13 Batch 30 Loss 3.5649\n",
            "Epoch 13 Loss 3.3514\n",
            "Time taken for 1 epoch 4.2328877449035645 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 3.0343\n",
            "Epoch 14 Batch 10 Loss 3.0021\n",
            "Epoch 14 Batch 20 Loss 3.1938\n",
            "Epoch 14 Batch 30 Loss 3.1988\n",
            "Epoch 14 Loss 3.2128\n",
            "Time taken for 1 epoch 4.3522629737854 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.9295\n",
            "Epoch 15 Batch 10 Loss 3.0468\n",
            "Epoch 15 Batch 20 Loss 3.0296\n",
            "Epoch 15 Batch 30 Loss 3.1468\n",
            "Epoch 15 Loss 3.0840\n",
            "Time taken for 1 epoch 4.235396862030029 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.9529\n",
            "Epoch 16 Batch 10 Loss 2.9751\n",
            "Epoch 16 Batch 20 Loss 3.2319\n",
            "Epoch 16 Batch 30 Loss 2.9086\n",
            "Epoch 16 Loss 2.9583\n",
            "Time taken for 1 epoch 4.443876028060913 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 3.0503\n",
            "Epoch 17 Batch 10 Loss 2.9362\n",
            "Epoch 17 Batch 20 Loss 2.9240\n",
            "Epoch 17 Batch 30 Loss 2.7282\n",
            "Epoch 17 Loss 2.8760\n",
            "Time taken for 1 epoch 4.348599433898926 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.7203\n",
            "Epoch 18 Batch 10 Loss 2.6686\n",
            "Epoch 18 Batch 20 Loss 2.7818\n",
            "Epoch 18 Batch 30 Loss 2.8160\n",
            "Epoch 18 Loss 2.7723\n",
            "Time taken for 1 epoch 4.463788032531738 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.6270\n",
            "Epoch 19 Batch 10 Loss 2.6600\n",
            "Epoch 19 Batch 20 Loss 2.4139\n",
            "Epoch 19 Batch 30 Loss 2.6222\n",
            "Epoch 19 Loss 2.6564\n",
            "Time taken for 1 epoch 4.332776784896851 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.5882\n",
            "Epoch 20 Batch 10 Loss 2.5875\n",
            "Epoch 20 Batch 20 Loss 2.4002\n",
            "Epoch 20 Batch 30 Loss 2.4823\n",
            "Epoch 20 Loss 2.5661\n",
            "Time taken for 1 epoch 4.453262567520142 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 2.4836\n",
            "Epoch 21 Batch 10 Loss 2.2464\n",
            "Epoch 21 Batch 20 Loss 2.5483\n",
            "Epoch 21 Batch 30 Loss 2.4457\n",
            "Epoch 21 Loss 2.4628\n",
            "Time taken for 1 epoch 4.343839883804321 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 2.3051\n",
            "Epoch 22 Batch 10 Loss 2.2528\n",
            "Epoch 22 Batch 20 Loss 2.3478\n",
            "Epoch 22 Batch 30 Loss 2.2685\n",
            "Epoch 22 Loss 2.3553\n",
            "Time taken for 1 epoch 4.414409160614014 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 2.1857\n",
            "Epoch 23 Batch 10 Loss 2.3175\n",
            "Epoch 23 Batch 20 Loss 2.1539\n",
            "Epoch 23 Batch 30 Loss 2.4471\n",
            "Epoch 23 Loss 2.2611\n",
            "Time taken for 1 epoch 4.249554634094238 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 2.2627\n",
            "Epoch 24 Batch 10 Loss 1.9863\n",
            "Epoch 24 Batch 20 Loss 2.1430\n",
            "Epoch 24 Batch 30 Loss 2.2817\n",
            "Epoch 24 Loss 2.1609\n",
            "Time taken for 1 epoch 4.401920557022095 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 2.0554\n",
            "Epoch 25 Batch 10 Loss 2.0987\n",
            "Epoch 25 Batch 20 Loss 2.0450\n",
            "Epoch 25 Batch 30 Loss 2.2532\n",
            "Epoch 25 Loss 2.0844\n",
            "Time taken for 1 epoch 4.26056170463562 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.9395\n",
            "Epoch 26 Batch 10 Loss 1.8762\n",
            "Epoch 26 Batch 20 Loss 2.0925\n",
            "Epoch 26 Batch 30 Loss 2.1274\n",
            "Epoch 26 Loss 1.9961\n",
            "Time taken for 1 epoch 4.438984155654907 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.6321\n",
            "Epoch 27 Batch 10 Loss 1.8292\n",
            "Epoch 27 Batch 20 Loss 1.9085\n",
            "Epoch 27 Batch 30 Loss 1.8838\n",
            "Epoch 27 Loss 1.9019\n",
            "Time taken for 1 epoch 4.2833473682403564 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.7435\n",
            "Epoch 28 Batch 10 Loss 1.8057\n",
            "Epoch 28 Batch 20 Loss 1.6407\n",
            "Epoch 28 Batch 30 Loss 1.9063\n",
            "Epoch 28 Loss 1.8064\n",
            "Time taken for 1 epoch 4.3805832862854 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.5900\n",
            "Epoch 29 Batch 10 Loss 1.6543\n",
            "Epoch 29 Batch 20 Loss 1.8744\n",
            "Epoch 29 Batch 30 Loss 1.6791\n",
            "Epoch 29 Loss 1.7310\n",
            "Time taken for 1 epoch 4.283153772354126 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.5268\n",
            "Epoch 30 Batch 10 Loss 1.6110\n",
            "Epoch 30 Batch 20 Loss 1.7851\n",
            "Epoch 30 Batch 30 Loss 1.9314\n",
            "Epoch 30 Loss 1.6583\n",
            "Time taken for 1 epoch 4.443946599960327 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.2875\n",
            "Epoch 31 Batch 10 Loss 1.4370\n",
            "Epoch 31 Batch 20 Loss 1.8134\n",
            "Epoch 31 Batch 30 Loss 1.7572\n",
            "Epoch 31 Loss 1.5760\n",
            "Time taken for 1 epoch 4.269330978393555 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.3892\n",
            "Epoch 32 Batch 10 Loss 1.4740\n",
            "Epoch 32 Batch 20 Loss 1.5307\n",
            "Epoch 32 Batch 30 Loss 1.6222\n",
            "Epoch 32 Loss 1.4996\n",
            "Time taken for 1 epoch 4.416745901107788 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.3350\n",
            "Epoch 33 Batch 10 Loss 1.5255\n",
            "Epoch 33 Batch 20 Loss 1.3988\n",
            "Epoch 33 Batch 30 Loss 1.5045\n",
            "Epoch 33 Loss 1.4238\n",
            "Time taken for 1 epoch 4.2740724086761475 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.2164\n",
            "Epoch 34 Batch 10 Loss 1.3266\n",
            "Epoch 34 Batch 20 Loss 1.5492\n",
            "Epoch 34 Batch 30 Loss 1.1695\n",
            "Epoch 34 Loss 1.3436\n",
            "Time taken for 1 epoch 4.426084280014038 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.0822\n",
            "Epoch 35 Batch 10 Loss 1.2216\n",
            "Epoch 35 Batch 20 Loss 1.2631\n",
            "Epoch 35 Batch 30 Loss 1.3495\n",
            "Epoch 35 Loss 1.2865\n",
            "Time taken for 1 epoch 4.294866561889648 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.2780\n",
            "Epoch 36 Batch 10 Loss 1.1425\n",
            "Epoch 36 Batch 20 Loss 1.1867\n",
            "Epoch 36 Batch 30 Loss 1.3340\n",
            "Epoch 36 Loss 1.2185\n",
            "Time taken for 1 epoch 4.388407230377197 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 1.0315\n",
            "Epoch 37 Batch 10 Loss 1.1201\n",
            "Epoch 37 Batch 20 Loss 1.0540\n",
            "Epoch 37 Batch 30 Loss 1.1297\n",
            "Epoch 37 Loss 1.1597\n",
            "Time taken for 1 epoch 4.313027620315552 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.0695\n",
            "Epoch 38 Batch 10 Loss 1.0674\n",
            "Epoch 38 Batch 20 Loss 1.1518\n",
            "Epoch 38 Batch 30 Loss 1.3705\n",
            "Epoch 38 Loss 1.0978\n",
            "Time taken for 1 epoch 4.408486604690552 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.9559\n",
            "Epoch 39 Batch 10 Loss 1.0616\n",
            "Epoch 39 Batch 20 Loss 1.0491\n",
            "Epoch 39 Batch 30 Loss 1.0210\n",
            "Epoch 39 Loss 1.0425\n",
            "Time taken for 1 epoch 4.320624351501465 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.9800\n",
            "Epoch 40 Batch 10 Loss 1.0059\n",
            "Epoch 40 Batch 20 Loss 0.9186\n",
            "Epoch 40 Batch 30 Loss 0.9910\n",
            "Epoch 40 Loss 0.9837\n",
            "Time taken for 1 epoch 4.470028400421143 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.0590\n",
            "Epoch 41 Batch 10 Loss 0.9466\n",
            "Epoch 41 Batch 20 Loss 0.9947\n",
            "Epoch 41 Batch 30 Loss 1.0371\n",
            "Epoch 41 Loss 0.9246\n",
            "Time taken for 1 epoch 4.4079999923706055 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.8542\n",
            "Epoch 42 Batch 10 Loss 0.8454\n",
            "Epoch 42 Batch 20 Loss 0.9074\n",
            "Epoch 42 Batch 30 Loss 0.8862\n",
            "Epoch 42 Loss 0.8883\n",
            "Time taken for 1 epoch 4.523907661437988 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.8916\n",
            "Epoch 43 Batch 10 Loss 0.8352\n",
            "Epoch 43 Batch 20 Loss 0.8727\n",
            "Epoch 43 Batch 30 Loss 0.8295\n",
            "Epoch 43 Loss 0.8391\n",
            "Time taken for 1 epoch 4.408820629119873 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.6130\n",
            "Epoch 44 Batch 10 Loss 0.7680\n",
            "Epoch 44 Batch 20 Loss 0.7609\n",
            "Epoch 44 Batch 30 Loss 0.7841\n",
            "Epoch 44 Loss 0.7958\n",
            "Time taken for 1 epoch 4.509317874908447 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.7383\n",
            "Epoch 45 Batch 10 Loss 0.8728\n",
            "Epoch 45 Batch 20 Loss 0.8504\n",
            "Epoch 45 Batch 30 Loss 0.7733\n",
            "Epoch 45 Loss 0.7592\n",
            "Time taken for 1 epoch 4.363332509994507 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.6434\n",
            "Epoch 46 Batch 10 Loss 0.6691\n",
            "Epoch 46 Batch 20 Loss 0.7739\n",
            "Epoch 46 Batch 30 Loss 0.6732\n",
            "Epoch 46 Loss 0.7054\n",
            "Time taken for 1 epoch 4.456859588623047 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.6641\n",
            "Epoch 47 Batch 10 Loss 0.7272\n",
            "Epoch 47 Batch 20 Loss 0.6679\n",
            "Epoch 47 Batch 30 Loss 0.6118\n",
            "Epoch 47 Loss 0.6741\n",
            "Time taken for 1 epoch 4.297303915023804 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.6794\n",
            "Epoch 48 Batch 10 Loss 0.6944\n",
            "Epoch 48 Batch 20 Loss 0.6865\n",
            "Epoch 48 Batch 30 Loss 0.5575\n",
            "Epoch 48 Loss 0.6257\n",
            "Time taken for 1 epoch 4.530975818634033 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.5611\n",
            "Epoch 49 Batch 10 Loss 0.5954\n",
            "Epoch 49 Batch 20 Loss 0.6368\n",
            "Epoch 49 Batch 30 Loss 0.5141\n",
            "Epoch 49 Loss 0.5887\n",
            "Time taken for 1 epoch 4.352789878845215 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.5002\n",
            "Epoch 50 Batch 10 Loss 0.5468\n",
            "Epoch 50 Batch 20 Loss 0.5125\n",
            "Epoch 50 Batch 30 Loss 0.6266\n",
            "Epoch 50 Loss 0.5503\n",
            "Time taken for 1 epoch 4.395866394042969 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.5404\n",
            "Epoch 51 Batch 10 Loss 0.5345\n",
            "Epoch 51 Batch 20 Loss 0.5890\n",
            "Epoch 51 Batch 30 Loss 0.4377\n",
            "Epoch 51 Loss 0.5152\n",
            "Time taken for 1 epoch 4.284107446670532 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.4374\n",
            "Epoch 52 Batch 10 Loss 0.4445\n",
            "Epoch 52 Batch 20 Loss 0.4658\n",
            "Epoch 52 Batch 30 Loss 0.4776\n",
            "Epoch 52 Loss 0.4881\n",
            "Time taken for 1 epoch 4.474653244018555 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.4201\n",
            "Epoch 53 Batch 10 Loss 0.4776\n",
            "Epoch 53 Batch 20 Loss 0.4921\n",
            "Epoch 53 Batch 30 Loss 0.4588\n",
            "Epoch 53 Loss 0.4604\n",
            "Time taken for 1 epoch 4.392584323883057 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.2910\n",
            "Epoch 54 Batch 10 Loss 0.5004\n",
            "Epoch 54 Batch 20 Loss 0.4394\n",
            "Epoch 54 Batch 30 Loss 0.4253\n",
            "Epoch 54 Loss 0.4289\n",
            "Time taken for 1 epoch 4.43189263343811 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2932\n",
            "Epoch 55 Batch 10 Loss 0.3965\n",
            "Epoch 55 Batch 20 Loss 0.3801\n",
            "Epoch 55 Batch 30 Loss 0.4940\n",
            "Epoch 55 Loss 0.4012\n",
            "Time taken for 1 epoch 4.295053243637085 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.3260\n",
            "Epoch 56 Batch 10 Loss 0.4108\n",
            "Epoch 56 Batch 20 Loss 0.4254\n",
            "Epoch 56 Batch 30 Loss 0.4180\n",
            "Epoch 56 Loss 0.3814\n",
            "Time taken for 1 epoch 4.463009595870972 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3375\n",
            "Epoch 57 Batch 10 Loss 0.3275\n",
            "Epoch 57 Batch 20 Loss 0.3168\n",
            "Epoch 57 Batch 30 Loss 0.3838\n",
            "Epoch 57 Loss 0.3535\n",
            "Time taken for 1 epoch 4.28812575340271 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.3238\n",
            "Epoch 58 Batch 10 Loss 0.3342\n",
            "Epoch 58 Batch 20 Loss 0.3259\n",
            "Epoch 58 Batch 30 Loss 0.3722\n",
            "Epoch 58 Loss 0.3312\n",
            "Time taken for 1 epoch 4.455145359039307 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.3229\n",
            "Epoch 59 Batch 10 Loss 0.3722\n",
            "Epoch 59 Batch 20 Loss 0.3571\n",
            "Epoch 59 Batch 30 Loss 0.3256\n",
            "Epoch 59 Loss 0.3053\n",
            "Time taken for 1 epoch 4.2868053913116455 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.2543\n",
            "Epoch 60 Batch 10 Loss 0.3365\n",
            "Epoch 60 Batch 20 Loss 0.2632\n",
            "Epoch 60 Batch 30 Loss 0.3461\n",
            "Epoch 60 Loss 0.2848\n",
            "Time taken for 1 epoch 4.490771532058716 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2204\n",
            "Epoch 61 Batch 10 Loss 0.2752\n",
            "Epoch 61 Batch 20 Loss 0.2555\n",
            "Epoch 61 Batch 30 Loss 0.2703\n",
            "Epoch 61 Loss 0.2612\n",
            "Time taken for 1 epoch 4.288222074508667 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.2339\n",
            "Epoch 62 Batch 10 Loss 0.2907\n",
            "Epoch 62 Batch 20 Loss 0.2513\n",
            "Epoch 62 Batch 30 Loss 0.2471\n",
            "Epoch 62 Loss 0.2432\n",
            "Time taken for 1 epoch 4.491094589233398 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.1983\n",
            "Epoch 63 Batch 10 Loss 0.1984\n",
            "Epoch 63 Batch 20 Loss 0.2087\n",
            "Epoch 63 Batch 30 Loss 0.2228\n",
            "Epoch 63 Loss 0.2283\n",
            "Time taken for 1 epoch 4.346703052520752 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2054\n",
            "Epoch 64 Batch 10 Loss 0.2096\n",
            "Epoch 64 Batch 20 Loss 0.2168\n",
            "Epoch 64 Batch 30 Loss 0.1937\n",
            "Epoch 64 Loss 0.2194\n",
            "Time taken for 1 epoch 4.474832773208618 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2234\n",
            "Epoch 65 Batch 10 Loss 0.1741\n",
            "Epoch 65 Batch 20 Loss 0.1967\n",
            "Epoch 65 Batch 30 Loss 0.1728\n",
            "Epoch 65 Loss 0.2067\n",
            "Time taken for 1 epoch 4.3737382888793945 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.1548\n",
            "Epoch 66 Batch 10 Loss 0.1849\n",
            "Epoch 66 Batch 20 Loss 0.2355\n",
            "Epoch 66 Batch 30 Loss 0.2317\n",
            "Epoch 66 Loss 0.1952\n",
            "Time taken for 1 epoch 4.547378301620483 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1613\n",
            "Epoch 67 Batch 10 Loss 0.1393\n",
            "Epoch 67 Batch 20 Loss 0.1908\n",
            "Epoch 67 Batch 30 Loss 0.1781\n",
            "Epoch 67 Loss 0.1814\n",
            "Time taken for 1 epoch 4.398728370666504 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.1519\n",
            "Epoch 68 Batch 10 Loss 0.1708\n",
            "Epoch 68 Batch 20 Loss 0.1630\n",
            "Epoch 68 Batch 30 Loss 0.2061\n",
            "Epoch 68 Loss 0.1733\n",
            "Time taken for 1 epoch 4.547379732131958 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1477\n",
            "Epoch 69 Batch 10 Loss 0.2038\n",
            "Epoch 69 Batch 20 Loss 0.2228\n",
            "Epoch 69 Batch 30 Loss 0.2103\n",
            "Epoch 69 Loss 0.1669\n",
            "Time taken for 1 epoch 4.379473924636841 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1491\n",
            "Epoch 70 Batch 10 Loss 0.1596\n",
            "Epoch 70 Batch 20 Loss 0.1357\n",
            "Epoch 70 Batch 30 Loss 0.1367\n",
            "Epoch 70 Loss 0.1508\n",
            "Time taken for 1 epoch 4.553704500198364 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1596\n",
            "Epoch 71 Batch 10 Loss 0.1252\n",
            "Epoch 71 Batch 20 Loss 0.1370\n",
            "Epoch 71 Batch 30 Loss 0.1185\n",
            "Epoch 71 Loss 0.1428\n",
            "Time taken for 1 epoch 4.341677904129028 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.1178\n",
            "Epoch 72 Batch 10 Loss 0.1287\n",
            "Epoch 72 Batch 20 Loss 0.1470\n",
            "Epoch 72 Batch 30 Loss 0.1317\n",
            "Epoch 72 Loss 0.1348\n",
            "Time taken for 1 epoch 4.424211502075195 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1331\n",
            "Epoch 73 Batch 10 Loss 0.1187\n",
            "Epoch 73 Batch 20 Loss 0.1115\n",
            "Epoch 73 Batch 30 Loss 0.1198\n",
            "Epoch 73 Loss 0.1276\n",
            "Time taken for 1 epoch 4.3390679359436035 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1271\n",
            "Epoch 74 Batch 10 Loss 0.0899\n",
            "Epoch 74 Batch 20 Loss 0.1562\n",
            "Epoch 74 Batch 30 Loss 0.1219\n",
            "Epoch 74 Loss 0.1202\n",
            "Time taken for 1 epoch 4.528352975845337 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1226\n",
            "Epoch 75 Batch 10 Loss 0.1309\n",
            "Epoch 75 Batch 20 Loss 0.1175\n",
            "Epoch 75 Batch 30 Loss 0.1169\n",
            "Epoch 75 Loss 0.1112\n",
            "Time taken for 1 epoch 4.342715501785278 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.1000\n",
            "Epoch 76 Batch 10 Loss 0.0893\n",
            "Epoch 76 Batch 20 Loss 0.1043\n",
            "Epoch 76 Batch 30 Loss 0.1301\n",
            "Epoch 76 Loss 0.1043\n",
            "Time taken for 1 epoch 4.484547138214111 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0859\n",
            "Epoch 77 Batch 10 Loss 0.1110\n",
            "Epoch 77 Batch 20 Loss 0.0885\n",
            "Epoch 77 Batch 30 Loss 0.1056\n",
            "Epoch 77 Loss 0.1003\n",
            "Time taken for 1 epoch 4.314823389053345 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0936\n",
            "Epoch 78 Batch 10 Loss 0.0915\n",
            "Epoch 78 Batch 20 Loss 0.0931\n",
            "Epoch 78 Batch 30 Loss 0.0744\n",
            "Epoch 78 Loss 0.0947\n",
            "Time taken for 1 epoch 4.455689191818237 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0961\n",
            "Epoch 79 Batch 10 Loss 0.1076\n",
            "Epoch 79 Batch 20 Loss 0.0931\n",
            "Epoch 79 Batch 30 Loss 0.0904\n",
            "Epoch 79 Loss 0.0912\n",
            "Time taken for 1 epoch 4.368779182434082 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0816\n",
            "Epoch 80 Batch 10 Loss 0.0860\n",
            "Epoch 80 Batch 20 Loss 0.0840\n",
            "Epoch 80 Batch 30 Loss 0.0900\n",
            "Epoch 80 Loss 0.0890\n",
            "Time taken for 1 epoch 4.469585418701172 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0739\n",
            "Epoch 81 Batch 10 Loss 0.0729\n",
            "Epoch 81 Batch 20 Loss 0.0866\n",
            "Epoch 81 Batch 30 Loss 0.0956\n",
            "Epoch 81 Loss 0.0876\n",
            "Time taken for 1 epoch 4.324662685394287 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0758\n",
            "Epoch 82 Batch 10 Loss 0.0694\n",
            "Epoch 82 Batch 20 Loss 0.0697\n",
            "Epoch 82 Batch 30 Loss 0.0792\n",
            "Epoch 82 Loss 0.0850\n",
            "Time taken for 1 epoch 4.525585651397705 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0873\n",
            "Epoch 83 Batch 10 Loss 0.0678\n",
            "Epoch 83 Batch 20 Loss 0.0774\n",
            "Epoch 83 Batch 30 Loss 0.0994\n",
            "Epoch 83 Loss 0.0852\n",
            "Time taken for 1 epoch 4.322378396987915 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0853\n",
            "Epoch 84 Batch 10 Loss 0.0674\n",
            "Epoch 84 Batch 20 Loss 0.0833\n",
            "Epoch 84 Batch 30 Loss 0.0957\n",
            "Epoch 84 Loss 0.0854\n",
            "Time taken for 1 epoch 4.442970037460327 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0846\n",
            "Epoch 85 Batch 10 Loss 0.0639\n",
            "Epoch 85 Batch 20 Loss 0.0852\n",
            "Epoch 85 Batch 30 Loss 0.0972\n",
            "Epoch 85 Loss 0.0805\n",
            "Time taken for 1 epoch 4.313007354736328 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0728\n",
            "Epoch 86 Batch 10 Loss 0.0764\n",
            "Epoch 86 Batch 20 Loss 0.0632\n",
            "Epoch 86 Batch 30 Loss 0.0618\n",
            "Epoch 86 Loss 0.0779\n",
            "Time taken for 1 epoch 4.469659090042114 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0529\n",
            "Epoch 87 Batch 10 Loss 0.0652\n",
            "Epoch 87 Batch 20 Loss 0.0757\n",
            "Epoch 87 Batch 30 Loss 0.0858\n",
            "Epoch 87 Loss 0.0789\n",
            "Time taken for 1 epoch 4.31782078742981 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0628\n",
            "Epoch 88 Batch 10 Loss 0.0653\n",
            "Epoch 88 Batch 20 Loss 0.0537\n",
            "Epoch 88 Batch 30 Loss 0.0906\n",
            "Epoch 88 Loss 0.0788\n",
            "Time taken for 1 epoch 4.4399800300598145 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0828\n",
            "Epoch 89 Batch 10 Loss 0.1040\n",
            "Epoch 89 Batch 20 Loss 0.0867\n",
            "Epoch 89 Batch 30 Loss 0.0773\n",
            "Epoch 89 Loss 0.0801\n",
            "Time taken for 1 epoch 4.42589807510376 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0847\n",
            "Epoch 90 Batch 10 Loss 0.0927\n",
            "Epoch 90 Batch 20 Loss 0.0626\n",
            "Epoch 90 Batch 30 Loss 0.0697\n",
            "Epoch 90 Loss 0.0761\n",
            "Time taken for 1 epoch 4.520190954208374 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0614\n",
            "Epoch 91 Batch 10 Loss 0.0779\n",
            "Epoch 91 Batch 20 Loss 0.0681\n",
            "Epoch 91 Batch 30 Loss 0.0785\n",
            "Epoch 91 Loss 0.0708\n",
            "Time taken for 1 epoch 4.393453121185303 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0514\n",
            "Epoch 92 Batch 10 Loss 0.0621\n",
            "Epoch 92 Batch 20 Loss 0.0734\n",
            "Epoch 92 Batch 30 Loss 0.0775\n",
            "Epoch 92 Loss 0.0661\n",
            "Time taken for 1 epoch 4.5718982219696045 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0743\n",
            "Epoch 93 Batch 10 Loss 0.0601\n",
            "Epoch 93 Batch 20 Loss 0.0695\n",
            "Epoch 93 Batch 30 Loss 0.0769\n",
            "Epoch 93 Loss 0.0684\n",
            "Time taken for 1 epoch 4.431685924530029 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0813\n",
            "Epoch 94 Batch 10 Loss 0.0759\n",
            "Epoch 94 Batch 20 Loss 0.0592\n",
            "Epoch 94 Batch 30 Loss 0.0730\n",
            "Epoch 94 Loss 0.0684\n",
            "Time taken for 1 epoch 4.4834723472595215 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0855\n",
            "Epoch 95 Batch 10 Loss 0.0687\n",
            "Epoch 95 Batch 20 Loss 0.0813\n",
            "Epoch 95 Batch 30 Loss 0.0571\n",
            "Epoch 95 Loss 0.0632\n",
            "Time taken for 1 epoch 4.361027002334595 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0762\n",
            "Epoch 96 Batch 10 Loss 0.0599\n",
            "Epoch 96 Batch 20 Loss 0.0553\n",
            "Epoch 96 Batch 30 Loss 0.0536\n",
            "Epoch 96 Loss 0.0585\n",
            "Time taken for 1 epoch 4.460970878601074 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0558\n",
            "Epoch 97 Batch 10 Loss 0.0597\n",
            "Epoch 97 Batch 20 Loss 0.0505\n",
            "Epoch 97 Batch 30 Loss 0.0415\n",
            "Epoch 97 Loss 0.0541\n",
            "Time taken for 1 epoch 4.394656419754028 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0553\n",
            "Epoch 98 Batch 10 Loss 0.0670\n",
            "Epoch 98 Batch 20 Loss 0.0468\n",
            "Epoch 98 Batch 30 Loss 0.0609\n",
            "Epoch 98 Loss 0.0536\n",
            "Time taken for 1 epoch 4.4509289264678955 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0537\n",
            "Epoch 99 Batch 10 Loss 0.0478\n",
            "Epoch 99 Batch 20 Loss 0.0605\n",
            "Epoch 99 Batch 30 Loss 0.0600\n",
            "Epoch 99 Loss 0.0532\n",
            "Time taken for 1 epoch 4.352602958679199 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0517\n",
            "Epoch 100 Batch 10 Loss 0.0600\n",
            "Epoch 100 Batch 20 Loss 0.0480\n",
            "Epoch 100 Batch 30 Loss 0.0390\n",
            "Epoch 100 Loss 0.0499\n",
            "Time taken for 1 epoch 4.447805404663086 sec\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.0460\n",
            "Epoch 101 Batch 10 Loss 0.0485\n",
            "Epoch 101 Batch 20 Loss 0.0576\n",
            "Epoch 101 Batch 30 Loss 0.0360\n",
            "Epoch 101 Loss 0.0478\n",
            "Time taken for 1 epoch 4.3521106243133545 sec\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.0472\n",
            "Epoch 102 Batch 10 Loss 0.0524\n",
            "Epoch 102 Batch 20 Loss 0.0430\n",
            "Epoch 102 Batch 30 Loss 0.0536\n",
            "Epoch 102 Loss 0.0433\n",
            "Time taken for 1 epoch 4.437408685684204 sec\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0322\n",
            "Epoch 103 Batch 10 Loss 0.0437\n",
            "Epoch 103 Batch 20 Loss 0.0458\n",
            "Epoch 103 Batch 30 Loss 0.0445\n",
            "Epoch 103 Loss 0.0426\n",
            "Time taken for 1 epoch 4.340782880783081 sec\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.0410\n",
            "Epoch 104 Batch 10 Loss 0.0360\n",
            "Epoch 104 Batch 20 Loss 0.0396\n",
            "Epoch 104 Batch 30 Loss 0.0569\n",
            "Epoch 104 Loss 0.0414\n",
            "Time taken for 1 epoch 4.490597248077393 sec\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.0367\n",
            "Epoch 105 Batch 10 Loss 0.0656\n",
            "Epoch 105 Batch 20 Loss 0.0257\n",
            "Epoch 105 Batch 30 Loss 0.0501\n",
            "Epoch 105 Loss 0.0408\n",
            "Time taken for 1 epoch 4.345747232437134 sec\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.0377\n",
            "Epoch 106 Batch 10 Loss 0.0399\n",
            "Epoch 106 Batch 20 Loss 0.0395\n",
            "Epoch 106 Batch 30 Loss 0.0492\n",
            "Epoch 106 Loss 0.0412\n",
            "Time taken for 1 epoch 4.4267120361328125 sec\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.0421\n",
            "Epoch 107 Batch 10 Loss 0.0319\n",
            "Epoch 107 Batch 20 Loss 0.0294\n",
            "Epoch 107 Batch 30 Loss 0.0289\n",
            "Epoch 107 Loss 0.0400\n",
            "Time taken for 1 epoch 4.334707975387573 sec\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.0342\n",
            "Epoch 108 Batch 10 Loss 0.0358\n",
            "Epoch 108 Batch 20 Loss 0.0298\n",
            "Epoch 108 Batch 30 Loss 0.0448\n",
            "Epoch 108 Loss 0.0382\n",
            "Time taken for 1 epoch 4.479345798492432 sec\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.0340\n",
            "Epoch 109 Batch 10 Loss 0.0367\n",
            "Epoch 109 Batch 20 Loss 0.0414\n",
            "Epoch 109 Batch 30 Loss 0.0419\n",
            "Epoch 109 Loss 0.0384\n",
            "Time taken for 1 epoch 4.35569953918457 sec\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.0230\n",
            "Epoch 110 Batch 10 Loss 0.0252\n",
            "Epoch 110 Batch 20 Loss 0.0443\n",
            "Epoch 110 Batch 30 Loss 0.0351\n",
            "Epoch 110 Loss 0.0363\n",
            "Time taken for 1 epoch 4.473159074783325 sec\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.0194\n",
            "Epoch 111 Batch 10 Loss 0.0247\n",
            "Epoch 111 Batch 20 Loss 0.0267\n",
            "Epoch 111 Batch 30 Loss 0.0293\n",
            "Epoch 111 Loss 0.0334\n",
            "Time taken for 1 epoch 4.311068058013916 sec\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.0306\n",
            "Epoch 112 Batch 10 Loss 0.0283\n",
            "Epoch 112 Batch 20 Loss 0.0429\n",
            "Epoch 112 Batch 30 Loss 0.0275\n",
            "Epoch 112 Loss 0.0332\n",
            "Time taken for 1 epoch 4.486212968826294 sec\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.0309\n",
            "Epoch 113 Batch 10 Loss 0.0330\n",
            "Epoch 113 Batch 20 Loss 0.0347\n",
            "Epoch 113 Batch 30 Loss 0.0341\n",
            "Epoch 113 Loss 0.0325\n",
            "Time taken for 1 epoch 4.419809103012085 sec\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.0302\n",
            "Epoch 114 Batch 10 Loss 0.0217\n",
            "Epoch 114 Batch 20 Loss 0.0359\n",
            "Epoch 114 Batch 30 Loss 0.0409\n",
            "Epoch 114 Loss 0.0329\n",
            "Time taken for 1 epoch 4.542701482772827 sec\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.0266\n",
            "Epoch 115 Batch 10 Loss 0.0299\n",
            "Epoch 115 Batch 20 Loss 0.0397\n",
            "Epoch 115 Batch 30 Loss 0.0207\n",
            "Epoch 115 Loss 0.0317\n",
            "Time taken for 1 epoch 4.4367406368255615 sec\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.0523\n",
            "Epoch 116 Batch 10 Loss 0.0450\n",
            "Epoch 116 Batch 20 Loss 0.0384\n",
            "Epoch 116 Batch 30 Loss 0.0388\n",
            "Epoch 116 Loss 0.0339\n",
            "Time taken for 1 epoch 4.5304176807403564 sec\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.0429\n",
            "Epoch 117 Batch 10 Loss 0.0315\n",
            "Epoch 117 Batch 20 Loss 0.0612\n",
            "Epoch 117 Batch 30 Loss 0.0302\n",
            "Epoch 117 Loss 0.0352\n",
            "Time taken for 1 epoch 4.3950254917144775 sec\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.0390\n",
            "Epoch 118 Batch 10 Loss 0.0513\n",
            "Epoch 118 Batch 20 Loss 0.0406\n",
            "Epoch 118 Batch 30 Loss 0.0484\n",
            "Epoch 118 Loss 0.0383\n",
            "Time taken for 1 epoch 4.532434940338135 sec\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.0317\n",
            "Epoch 119 Batch 10 Loss 0.0487\n",
            "Epoch 119 Batch 20 Loss 0.0488\n",
            "Epoch 119 Batch 30 Loss 0.0462\n",
            "Epoch 119 Loss 0.0504\n",
            "Time taken for 1 epoch 4.340891599655151 sec\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0441\n",
            "Epoch 120 Batch 10 Loss 0.0537\n",
            "Epoch 120 Batch 20 Loss 0.0552\n",
            "Epoch 120 Batch 30 Loss 0.0501\n",
            "Epoch 120 Loss 0.0644\n",
            "Time taken for 1 epoch 4.446375131607056 sec\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.0732\n",
            "Epoch 121 Batch 10 Loss 0.0948\n",
            "Epoch 121 Batch 20 Loss 0.0979\n",
            "Epoch 121 Batch 30 Loss 0.0932\n",
            "Epoch 121 Loss 0.0995\n",
            "Time taken for 1 epoch 4.32600998878479 sec\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.1293\n",
            "Epoch 122 Batch 10 Loss 0.1376\n",
            "Epoch 122 Batch 20 Loss 0.1739\n",
            "Epoch 122 Batch 30 Loss 0.1860\n",
            "Epoch 122 Loss 0.1552\n",
            "Time taken for 1 epoch 4.454132080078125 sec\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.1334\n",
            "Epoch 123 Batch 10 Loss 0.1380\n",
            "Epoch 123 Batch 20 Loss 0.1898\n",
            "Epoch 123 Batch 30 Loss 0.1845\n",
            "Epoch 123 Loss 0.1779\n",
            "Time taken for 1 epoch 4.3192667961120605 sec\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.1309\n",
            "Epoch 124 Batch 10 Loss 0.1689\n",
            "Epoch 124 Batch 20 Loss 0.1814\n",
            "Epoch 124 Batch 30 Loss 0.1962\n",
            "Epoch 124 Loss 0.1619\n",
            "Time taken for 1 epoch 4.500559091567993 sec\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.1000\n",
            "Epoch 125 Batch 10 Loss 0.1053\n",
            "Epoch 125 Batch 20 Loss 0.1452\n",
            "Epoch 125 Batch 30 Loss 0.1236\n",
            "Epoch 125 Loss 0.1239\n",
            "Time taken for 1 epoch 4.364963054656982 sec\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.0938\n",
            "Epoch 126 Batch 10 Loss 0.0837\n",
            "Epoch 126 Batch 20 Loss 0.1333\n",
            "Epoch 126 Batch 30 Loss 0.0969\n",
            "Epoch 126 Loss 0.0941\n",
            "Time taken for 1 epoch 4.451327562332153 sec\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.0616\n",
            "Epoch 127 Batch 10 Loss 0.0551\n",
            "Epoch 127 Batch 20 Loss 0.0705\n",
            "Epoch 127 Batch 30 Loss 0.0563\n",
            "Epoch 127 Loss 0.0724\n",
            "Time taken for 1 epoch 4.388840436935425 sec\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.0600\n",
            "Epoch 128 Batch 10 Loss 0.0770\n",
            "Epoch 128 Batch 20 Loss 0.0474\n",
            "Epoch 128 Batch 30 Loss 0.0453\n",
            "Epoch 128 Loss 0.0585\n",
            "Time taken for 1 epoch 4.523963928222656 sec\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0476\n",
            "Epoch 129 Batch 10 Loss 0.0359\n",
            "Epoch 129 Batch 20 Loss 0.0362\n",
            "Epoch 129 Batch 30 Loss 0.0610\n",
            "Epoch 129 Loss 0.0436\n",
            "Time taken for 1 epoch 4.368519067764282 sec\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.0390\n",
            "Epoch 130 Batch 10 Loss 0.0365\n",
            "Epoch 130 Batch 20 Loss 0.0511\n",
            "Epoch 130 Batch 30 Loss 0.0333\n",
            "Epoch 130 Loss 0.0361\n",
            "Time taken for 1 epoch 4.442591667175293 sec\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0358\n",
            "Epoch 131 Batch 10 Loss 0.0239\n",
            "Epoch 131 Batch 20 Loss 0.0353\n",
            "Epoch 131 Batch 30 Loss 0.0228\n",
            "Epoch 131 Loss 0.0315\n",
            "Time taken for 1 epoch 4.299590587615967 sec\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.0162\n",
            "Epoch 132 Batch 10 Loss 0.0145\n",
            "Epoch 132 Batch 20 Loss 0.0302\n",
            "Epoch 132 Batch 30 Loss 0.0316\n",
            "Epoch 132 Loss 0.0285\n",
            "Time taken for 1 epoch 4.486846446990967 sec\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0244\n",
            "Epoch 133 Batch 10 Loss 0.0277\n",
            "Epoch 133 Batch 20 Loss 0.0280\n",
            "Epoch 133 Batch 30 Loss 0.0235\n",
            "Epoch 133 Loss 0.0267\n",
            "Time taken for 1 epoch 4.30317234992981 sec\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.0198\n",
            "Epoch 134 Batch 10 Loss 0.0235\n",
            "Epoch 134 Batch 20 Loss 0.0187\n",
            "Epoch 134 Batch 30 Loss 0.0225\n",
            "Epoch 134 Loss 0.0252\n",
            "Time taken for 1 epoch 4.482303619384766 sec\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0245\n",
            "Epoch 135 Batch 10 Loss 0.0346\n",
            "Epoch 135 Batch 20 Loss 0.0289\n",
            "Epoch 135 Batch 30 Loss 0.0256\n",
            "Epoch 135 Loss 0.0239\n",
            "Time taken for 1 epoch 4.359488248825073 sec\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.0214\n",
            "Epoch 136 Batch 10 Loss 0.0227\n",
            "Epoch 136 Batch 20 Loss 0.0263\n",
            "Epoch 136 Batch 30 Loss 0.0222\n",
            "Epoch 136 Loss 0.0230\n",
            "Time taken for 1 epoch 4.544301271438599 sec\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0206\n",
            "Epoch 137 Batch 10 Loss 0.0203\n",
            "Epoch 137 Batch 20 Loss 0.0171\n",
            "Epoch 137 Batch 30 Loss 0.0262\n",
            "Epoch 137 Loss 0.0224\n",
            "Time taken for 1 epoch 4.391317844390869 sec\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0192\n",
            "Epoch 138 Batch 10 Loss 0.0240\n",
            "Epoch 138 Batch 20 Loss 0.0254\n",
            "Epoch 138 Batch 30 Loss 0.0169\n",
            "Epoch 138 Loss 0.0221\n",
            "Time taken for 1 epoch 4.502047538757324 sec\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.0167\n",
            "Epoch 139 Batch 10 Loss 0.0211\n",
            "Epoch 139 Batch 20 Loss 0.0236\n",
            "Epoch 139 Batch 30 Loss 0.0260\n",
            "Epoch 139 Loss 0.0215\n",
            "Time taken for 1 epoch 4.388144254684448 sec\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.0149\n",
            "Epoch 140 Batch 10 Loss 0.0436\n",
            "Epoch 140 Batch 20 Loss 0.0316\n",
            "Epoch 140 Batch 30 Loss 0.0240\n",
            "Epoch 140 Loss 0.0214\n",
            "Time taken for 1 epoch 4.504795074462891 sec\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0238\n",
            "Epoch 141 Batch 10 Loss 0.0146\n",
            "Epoch 141 Batch 20 Loss 0.0194\n",
            "Epoch 141 Batch 30 Loss 0.0252\n",
            "Epoch 141 Loss 0.0202\n",
            "Time taken for 1 epoch 4.407528638839722 sec\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.0145\n",
            "Epoch 142 Batch 10 Loss 0.0201\n",
            "Epoch 142 Batch 20 Loss 0.0108\n",
            "Epoch 142 Batch 30 Loss 0.0165\n",
            "Epoch 142 Loss 0.0202\n",
            "Time taken for 1 epoch 4.654088020324707 sec\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0171\n",
            "Epoch 143 Batch 10 Loss 0.0283\n",
            "Epoch 143 Batch 20 Loss 0.0304\n",
            "Epoch 143 Batch 30 Loss 0.0196\n",
            "Epoch 143 Loss 0.0198\n",
            "Time taken for 1 epoch 4.3248279094696045 sec\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.0150\n",
            "Epoch 144 Batch 10 Loss 0.0211\n",
            "Epoch 144 Batch 20 Loss 0.0229\n",
            "Epoch 144 Batch 30 Loss 0.0215\n",
            "Epoch 144 Loss 0.0206\n",
            "Time taken for 1 epoch 4.456972599029541 sec\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.0329\n",
            "Epoch 145 Batch 10 Loss 0.0243\n",
            "Epoch 145 Batch 20 Loss 0.0198\n",
            "Epoch 145 Batch 30 Loss 0.0144\n",
            "Epoch 145 Loss 0.0202\n",
            "Time taken for 1 epoch 4.314437389373779 sec\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.0261\n",
            "Epoch 146 Batch 10 Loss 0.0120\n",
            "Epoch 146 Batch 20 Loss 0.0248\n",
            "Epoch 146 Batch 30 Loss 0.0095\n",
            "Epoch 146 Loss 0.0194\n",
            "Time taken for 1 epoch 4.471491575241089 sec\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0166\n",
            "Epoch 147 Batch 10 Loss 0.0144\n",
            "Epoch 147 Batch 20 Loss 0.0227\n",
            "Epoch 147 Batch 30 Loss 0.0143\n",
            "Epoch 147 Loss 0.0186\n",
            "Time taken for 1 epoch 4.288654327392578 sec\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0125\n",
            "Epoch 148 Batch 10 Loss 0.0202\n",
            "Epoch 148 Batch 20 Loss 0.0182\n",
            "Epoch 148 Batch 30 Loss 0.0152\n",
            "Epoch 148 Loss 0.0186\n",
            "Time taken for 1 epoch 4.437654972076416 sec\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0138\n",
            "Epoch 149 Batch 10 Loss 0.0225\n",
            "Epoch 149 Batch 20 Loss 0.0240\n",
            "Epoch 149 Batch 30 Loss 0.0135\n",
            "Epoch 149 Loss 0.0179\n",
            "Time taken for 1 epoch 4.314049005508423 sec\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0131\n",
            "Epoch 150 Batch 10 Loss 0.0177\n",
            "Epoch 150 Batch 20 Loss 0.0109\n",
            "Epoch 150 Batch 30 Loss 0.0193\n",
            "Epoch 150 Loss 0.0190\n",
            "Time taken for 1 epoch 4.514714241027832 sec\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.0211\n",
            "Epoch 151 Batch 10 Loss 0.0233\n",
            "Epoch 151 Batch 20 Loss 0.0324\n",
            "Epoch 151 Batch 30 Loss 0.0116\n",
            "Epoch 151 Loss 0.0191\n",
            "Time taken for 1 epoch 4.311145067214966 sec\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.0203\n",
            "Epoch 152 Batch 10 Loss 0.0122\n",
            "Epoch 152 Batch 20 Loss 0.0155\n",
            "Epoch 152 Batch 30 Loss 0.0190\n",
            "Epoch 152 Loss 0.0191\n",
            "Time taken for 1 epoch 4.432352066040039 sec\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.0139\n",
            "Epoch 153 Batch 10 Loss 0.0315\n",
            "Epoch 153 Batch 20 Loss 0.0192\n",
            "Epoch 153 Batch 30 Loss 0.0141\n",
            "Epoch 153 Loss 0.0183\n",
            "Time taken for 1 epoch 4.354806900024414 sec\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.0323\n",
            "Epoch 154 Batch 10 Loss 0.0155\n",
            "Epoch 154 Batch 20 Loss 0.0328\n",
            "Epoch 154 Batch 30 Loss 0.0164\n",
            "Epoch 154 Loss 0.0193\n",
            "Time taken for 1 epoch 4.414336681365967 sec\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.0147\n",
            "Epoch 155 Batch 10 Loss 0.0206\n",
            "Epoch 155 Batch 20 Loss 0.0088\n",
            "Epoch 155 Batch 30 Loss 0.0184\n",
            "Epoch 155 Loss 0.0192\n",
            "Time taken for 1 epoch 4.331778049468994 sec\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.0181\n",
            "Epoch 156 Batch 10 Loss 0.0107\n",
            "Epoch 156 Batch 20 Loss 0.0097\n",
            "Epoch 156 Batch 30 Loss 0.0251\n",
            "Epoch 156 Loss 0.0191\n",
            "Time taken for 1 epoch 4.569684028625488 sec\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.0144\n",
            "Epoch 157 Batch 10 Loss 0.0199\n",
            "Epoch 157 Batch 20 Loss 0.0207\n",
            "Epoch 157 Batch 30 Loss 0.0164\n",
            "Epoch 157 Loss 0.0196\n",
            "Time taken for 1 epoch 4.336090564727783 sec\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.0184\n",
            "Epoch 158 Batch 10 Loss 0.0154\n",
            "Epoch 158 Batch 20 Loss 0.0278\n",
            "Epoch 158 Batch 30 Loss 0.0124\n",
            "Epoch 158 Loss 0.0181\n",
            "Time taken for 1 epoch 4.463370323181152 sec\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.0201\n",
            "Epoch 159 Batch 10 Loss 0.0153\n",
            "Epoch 159 Batch 20 Loss 0.0181\n",
            "Epoch 159 Batch 30 Loss 0.0155\n",
            "Epoch 159 Loss 0.0192\n",
            "Time taken for 1 epoch 4.329801559448242 sec\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.0116\n",
            "Epoch 160 Batch 10 Loss 0.0179\n",
            "Epoch 160 Batch 20 Loss 0.0274\n",
            "Epoch 160 Batch 30 Loss 0.0291\n",
            "Epoch 160 Loss 0.0187\n",
            "Time taken for 1 epoch 4.548959016799927 sec\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.0082\n",
            "Epoch 161 Batch 10 Loss 0.0242\n",
            "Epoch 161 Batch 20 Loss 0.0105\n",
            "Epoch 161 Batch 30 Loss 0.0192\n",
            "Epoch 161 Loss 0.0190\n",
            "Time taken for 1 epoch 4.432334899902344 sec\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.0203\n",
            "Epoch 162 Batch 10 Loss 0.0090\n",
            "Epoch 162 Batch 20 Loss 0.0211\n",
            "Epoch 162 Batch 30 Loss 0.0158\n",
            "Epoch 162 Loss 0.0191\n",
            "Time taken for 1 epoch 4.483196258544922 sec\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.0149\n",
            "Epoch 163 Batch 10 Loss 0.0461\n",
            "Epoch 163 Batch 20 Loss 0.0139\n",
            "Epoch 163 Batch 30 Loss 0.0161\n",
            "Epoch 163 Loss 0.0190\n",
            "Time taken for 1 epoch 4.3774473667144775 sec\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.0155\n",
            "Epoch 164 Batch 10 Loss 0.0140\n",
            "Epoch 164 Batch 20 Loss 0.0482\n",
            "Epoch 164 Batch 30 Loss 0.0212\n",
            "Epoch 164 Loss 0.0226\n",
            "Time taken for 1 epoch 4.518043279647827 sec\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0242\n",
            "Epoch 165 Batch 10 Loss 0.0139\n",
            "Epoch 165 Batch 20 Loss 0.0181\n",
            "Epoch 165 Batch 30 Loss 0.0264\n",
            "Epoch 165 Loss 0.0247\n",
            "Time taken for 1 epoch 4.422328948974609 sec\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.0174\n",
            "Epoch 166 Batch 10 Loss 0.0356\n",
            "Epoch 166 Batch 20 Loss 0.0693\n",
            "Epoch 166 Batch 30 Loss 0.0299\n",
            "Epoch 166 Loss 0.0362\n",
            "Time taken for 1 epoch 4.520718336105347 sec\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.0186\n",
            "Epoch 167 Batch 10 Loss 0.0318\n",
            "Epoch 167 Batch 20 Loss 0.0420\n",
            "Epoch 167 Batch 30 Loss 0.0374\n",
            "Epoch 167 Loss 0.0510\n",
            "Time taken for 1 epoch 4.314002275466919 sec\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.0525\n",
            "Epoch 168 Batch 10 Loss 0.0582\n",
            "Epoch 168 Batch 20 Loss 0.0985\n",
            "Epoch 168 Batch 30 Loss 0.0943\n",
            "Epoch 168 Loss 0.0892\n",
            "Time taken for 1 epoch 4.510466575622559 sec\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0933\n",
            "Epoch 169 Batch 10 Loss 0.1293\n",
            "Epoch 169 Batch 20 Loss 0.1515\n",
            "Epoch 169 Batch 30 Loss 0.1402\n",
            "Epoch 169 Loss 0.1373\n",
            "Time taken for 1 epoch 4.3195641040802 sec\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.1835\n",
            "Epoch 170 Batch 10 Loss 0.1976\n",
            "Epoch 170 Batch 20 Loss 0.1382\n",
            "Epoch 170 Batch 30 Loss 0.1644\n",
            "Epoch 170 Loss 0.1550\n",
            "Time taken for 1 epoch 4.448702335357666 sec\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.1285\n",
            "Epoch 171 Batch 10 Loss 0.1574\n",
            "Epoch 171 Batch 20 Loss 0.1869\n",
            "Epoch 171 Batch 30 Loss 0.1538\n",
            "Epoch 171 Loss 0.1574\n",
            "Time taken for 1 epoch 4.326627731323242 sec\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.1202\n",
            "Epoch 172 Batch 10 Loss 0.1499\n",
            "Epoch 172 Batch 20 Loss 0.1484\n",
            "Epoch 172 Batch 30 Loss 0.1151\n",
            "Epoch 172 Loss 0.1305\n",
            "Time taken for 1 epoch 4.508171558380127 sec\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0695\n",
            "Epoch 173 Batch 10 Loss 0.0925\n",
            "Epoch 173 Batch 20 Loss 0.1233\n",
            "Epoch 173 Batch 30 Loss 0.0802\n",
            "Epoch 173 Loss 0.0983\n",
            "Time taken for 1 epoch 4.295197486877441 sec\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.0874\n",
            "Epoch 174 Batch 10 Loss 0.0483\n",
            "Epoch 174 Batch 20 Loss 0.0672\n",
            "Epoch 174 Batch 30 Loss 0.0624\n",
            "Epoch 174 Loss 0.0688\n",
            "Time taken for 1 epoch 4.5481531620025635 sec\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.0393\n",
            "Epoch 175 Batch 10 Loss 0.0639\n",
            "Epoch 175 Batch 20 Loss 0.0470\n",
            "Epoch 175 Batch 30 Loss 0.0477\n",
            "Epoch 175 Loss 0.0518\n",
            "Time taken for 1 epoch 4.302610874176025 sec\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.0249\n",
            "Epoch 176 Batch 10 Loss 0.0418\n",
            "Epoch 176 Batch 20 Loss 0.0324\n",
            "Epoch 176 Batch 30 Loss 0.0537\n",
            "Epoch 176 Loss 0.0391\n",
            "Time taken for 1 epoch 4.423089981079102 sec\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.0490\n",
            "Epoch 177 Batch 10 Loss 0.0221\n",
            "Epoch 177 Batch 20 Loss 0.0391\n",
            "Epoch 177 Batch 30 Loss 0.0405\n",
            "Epoch 177 Loss 0.0340\n",
            "Time taken for 1 epoch 4.3185341358184814 sec\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.0383\n",
            "Epoch 178 Batch 10 Loss 0.0232\n",
            "Epoch 178 Batch 20 Loss 0.0334\n",
            "Epoch 178 Batch 30 Loss 0.0381\n",
            "Epoch 178 Loss 0.0293\n",
            "Time taken for 1 epoch 4.444795370101929 sec\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0304\n",
            "Epoch 179 Batch 10 Loss 0.0240\n",
            "Epoch 179 Batch 20 Loss 0.0333\n",
            "Epoch 179 Batch 30 Loss 0.0233\n",
            "Epoch 179 Loss 0.0261\n",
            "Time taken for 1 epoch 4.30636739730835 sec\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.0125\n",
            "Epoch 180 Batch 10 Loss 0.0148\n",
            "Epoch 180 Batch 20 Loss 0.0289\n",
            "Epoch 180 Batch 30 Loss 0.0136\n",
            "Epoch 180 Loss 0.0233\n",
            "Time taken for 1 epoch 4.513171434402466 sec\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0114\n",
            "Epoch 181 Batch 10 Loss 0.0215\n",
            "Epoch 181 Batch 20 Loss 0.0099\n",
            "Epoch 181 Batch 30 Loss 0.0132\n",
            "Epoch 181 Loss 0.0214\n",
            "Time taken for 1 epoch 4.355181694030762 sec\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.0121\n",
            "Epoch 182 Batch 10 Loss 0.0177\n",
            "Epoch 182 Batch 20 Loss 0.0262\n",
            "Epoch 182 Batch 30 Loss 0.0154\n",
            "Epoch 182 Loss 0.0203\n",
            "Time taken for 1 epoch 4.436725616455078 sec\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.0176\n",
            "Epoch 183 Batch 10 Loss 0.0178\n",
            "Epoch 183 Batch 20 Loss 0.0212\n",
            "Epoch 183 Batch 30 Loss 0.0218\n",
            "Epoch 183 Loss 0.0187\n",
            "Time taken for 1 epoch 4.314181089401245 sec\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.0127\n",
            "Epoch 184 Batch 10 Loss 0.0348\n",
            "Epoch 184 Batch 20 Loss 0.0333\n",
            "Epoch 184 Batch 30 Loss 0.0207\n",
            "Epoch 184 Loss 0.0185\n",
            "Time taken for 1 epoch 4.514130592346191 sec\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0147\n",
            "Epoch 185 Batch 10 Loss 0.0235\n",
            "Epoch 185 Batch 20 Loss 0.0163\n",
            "Epoch 185 Batch 30 Loss 0.0185\n",
            "Epoch 185 Loss 0.0171\n",
            "Time taken for 1 epoch 4.414923906326294 sec\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.0108\n",
            "Epoch 186 Batch 10 Loss 0.0200\n",
            "Epoch 186 Batch 20 Loss 0.0131\n",
            "Epoch 186 Batch 30 Loss 0.0208\n",
            "Epoch 186 Loss 0.0167\n",
            "Time taken for 1 epoch 4.485283851623535 sec\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0283\n",
            "Epoch 187 Batch 10 Loss 0.0144\n",
            "Epoch 187 Batch 20 Loss 0.0090\n",
            "Epoch 187 Batch 30 Loss 0.0063\n",
            "Epoch 187 Loss 0.0162\n",
            "Time taken for 1 epoch 4.3950135707855225 sec\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.0069\n",
            "Epoch 188 Batch 10 Loss 0.0149\n",
            "Epoch 188 Batch 20 Loss 0.0172\n",
            "Epoch 188 Batch 30 Loss 0.0161\n",
            "Epoch 188 Loss 0.0144\n",
            "Time taken for 1 epoch 4.5502848625183105 sec\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0141\n",
            "Epoch 189 Batch 10 Loss 0.0125\n",
            "Epoch 189 Batch 20 Loss 0.0136\n",
            "Epoch 189 Batch 30 Loss 0.0051\n",
            "Epoch 189 Loss 0.0146\n",
            "Time taken for 1 epoch 4.37831974029541 sec\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.0062\n",
            "Epoch 190 Batch 10 Loss 0.0128\n",
            "Epoch 190 Batch 20 Loss 0.0104\n",
            "Epoch 190 Batch 30 Loss 0.0125\n",
            "Epoch 190 Loss 0.0144\n",
            "Time taken for 1 epoch 4.632643938064575 sec\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0143\n",
            "Epoch 191 Batch 10 Loss 0.0164\n",
            "Epoch 191 Batch 20 Loss 0.0136\n",
            "Epoch 191 Batch 30 Loss 0.0107\n",
            "Epoch 191 Loss 0.0141\n",
            "Time taken for 1 epoch 4.354871988296509 sec\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.0197\n",
            "Epoch 192 Batch 10 Loss 0.0119\n",
            "Epoch 192 Batch 20 Loss 0.0334\n",
            "Epoch 192 Batch 30 Loss 0.0147\n",
            "Epoch 192 Loss 0.0147\n",
            "Time taken for 1 epoch 4.467951536178589 sec\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.0067\n",
            "Epoch 193 Batch 10 Loss 0.0082\n",
            "Epoch 193 Batch 20 Loss 0.0154\n",
            "Epoch 193 Batch 30 Loss 0.0159\n",
            "Epoch 193 Loss 0.0141\n",
            "Time taken for 1 epoch 4.34122896194458 sec\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.0094\n",
            "Epoch 194 Batch 10 Loss 0.0126\n",
            "Epoch 194 Batch 20 Loss 0.0153\n",
            "Epoch 194 Batch 30 Loss 0.0115\n",
            "Epoch 194 Loss 0.0139\n",
            "Time taken for 1 epoch 4.488758325576782 sec\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0092\n",
            "Epoch 195 Batch 10 Loss 0.0118\n",
            "Epoch 195 Batch 20 Loss 0.0196\n",
            "Epoch 195 Batch 30 Loss 0.0109\n",
            "Epoch 195 Loss 0.0134\n",
            "Time taken for 1 epoch 4.299652814865112 sec\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.0118\n",
            "Epoch 196 Batch 10 Loss 0.0159\n",
            "Epoch 196 Batch 20 Loss 0.0093\n",
            "Epoch 196 Batch 30 Loss 0.0045\n",
            "Epoch 196 Loss 0.0136\n",
            "Time taken for 1 epoch 4.458038330078125 sec\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0137\n",
            "Epoch 197 Batch 10 Loss 0.0156\n",
            "Epoch 197 Batch 20 Loss 0.0124\n",
            "Epoch 197 Batch 30 Loss 0.0152\n",
            "Epoch 197 Loss 0.0137\n",
            "Time taken for 1 epoch 4.316219329833984 sec\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.0119\n",
            "Epoch 198 Batch 10 Loss 0.0151\n",
            "Epoch 198 Batch 20 Loss 0.0144\n",
            "Epoch 198 Batch 30 Loss 0.0101\n",
            "Epoch 198 Loss 0.0131\n",
            "Time taken for 1 epoch 4.462519407272339 sec\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0095\n",
            "Epoch 199 Batch 10 Loss 0.0086\n",
            "Epoch 199 Batch 20 Loss 0.0028\n",
            "Epoch 199 Batch 30 Loss 0.0084\n",
            "Epoch 199 Loss 0.0125\n",
            "Time taken for 1 epoch 4.359191417694092 sec\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.0104\n",
            "Epoch 200 Batch 10 Loss 0.0172\n",
            "Epoch 200 Batch 20 Loss 0.0170\n",
            "Epoch 200 Batch 30 Loss 0.0189\n",
            "Epoch 200 Loss 0.0133\n",
            "Time taken for 1 epoch 4.555544137954712 sec\n",
            "\n",
            "Epoch 201 Batch 0 Loss 0.0144\n",
            "Epoch 201 Batch 10 Loss 0.0086\n",
            "Epoch 201 Batch 20 Loss 0.0100\n",
            "Epoch 201 Batch 30 Loss 0.0136\n",
            "Epoch 201 Loss 0.0125\n",
            "Time taken for 1 epoch 4.366060018539429 sec\n",
            "\n",
            "Epoch 202 Batch 0 Loss 0.0101\n",
            "Epoch 202 Batch 10 Loss 0.0086\n",
            "Epoch 202 Batch 20 Loss 0.0092\n",
            "Epoch 202 Batch 30 Loss 0.0262\n",
            "Epoch 202 Loss 0.0131\n",
            "Time taken for 1 epoch 4.478145599365234 sec\n",
            "\n",
            "Epoch 203 Batch 0 Loss 0.0142\n",
            "Epoch 203 Batch 10 Loss 0.0066\n",
            "Epoch 203 Batch 20 Loss 0.0106\n",
            "Epoch 203 Batch 30 Loss 0.0066\n",
            "Epoch 203 Loss 0.0123\n",
            "Time taken for 1 epoch 4.335252523422241 sec\n",
            "\n",
            "Epoch 204 Batch 0 Loss 0.0140\n",
            "Epoch 204 Batch 10 Loss 0.0252\n",
            "Epoch 204 Batch 20 Loss 0.0190\n",
            "Epoch 204 Batch 30 Loss 0.0074\n",
            "Epoch 204 Loss 0.0134\n",
            "Time taken for 1 epoch 4.474138021469116 sec\n",
            "\n",
            "Epoch 205 Batch 0 Loss 0.0267\n",
            "Epoch 205 Batch 10 Loss 0.0125\n",
            "Epoch 205 Batch 20 Loss 0.0094\n",
            "Epoch 205 Batch 30 Loss 0.0083\n",
            "Epoch 205 Loss 0.0126\n",
            "Time taken for 1 epoch 4.3574488162994385 sec\n",
            "\n",
            "Epoch 206 Batch 0 Loss 0.0026\n",
            "Epoch 206 Batch 10 Loss 0.0105\n",
            "Epoch 206 Batch 20 Loss 0.0103\n",
            "Epoch 206 Batch 30 Loss 0.0073\n",
            "Epoch 206 Loss 0.0125\n",
            "Time taken for 1 epoch 4.497470378875732 sec\n",
            "\n",
            "Epoch 207 Batch 0 Loss 0.0125\n",
            "Epoch 207 Batch 10 Loss 0.0138\n",
            "Epoch 207 Batch 20 Loss 0.0142\n",
            "Epoch 207 Batch 30 Loss 0.0074\n",
            "Epoch 207 Loss 0.0124\n",
            "Time taken for 1 epoch 4.333078622817993 sec\n",
            "\n",
            "Epoch 208 Batch 0 Loss 0.0164\n",
            "Epoch 208 Batch 10 Loss 0.0092\n",
            "Epoch 208 Batch 20 Loss 0.0069\n",
            "Epoch 208 Batch 30 Loss 0.0020\n",
            "Epoch 208 Loss 0.0123\n",
            "Time taken for 1 epoch 4.480935335159302 sec\n",
            "\n",
            "Epoch 209 Batch 0 Loss 0.0170\n",
            "Epoch 209 Batch 10 Loss 0.0080\n",
            "Epoch 209 Batch 20 Loss 0.0127\n",
            "Epoch 209 Batch 30 Loss 0.0103\n",
            "Epoch 209 Loss 0.0120\n",
            "Time taken for 1 epoch 4.399997711181641 sec\n",
            "\n",
            "Epoch 210 Batch 0 Loss 0.0191\n",
            "Epoch 210 Batch 10 Loss 0.0138\n",
            "Epoch 210 Batch 20 Loss 0.0261\n",
            "Epoch 210 Batch 30 Loss 0.0074\n",
            "Epoch 210 Loss 0.0125\n",
            "Time taken for 1 epoch 4.52249813079834 sec\n",
            "\n",
            "Epoch 211 Batch 0 Loss 0.0122\n",
            "Epoch 211 Batch 10 Loss 0.0214\n",
            "Epoch 211 Batch 20 Loss 0.0186\n",
            "Epoch 211 Batch 30 Loss 0.0059\n",
            "Epoch 211 Loss 0.0118\n",
            "Time taken for 1 epoch 4.432833671569824 sec\n",
            "\n",
            "Epoch 212 Batch 0 Loss 0.0085\n",
            "Epoch 212 Batch 10 Loss 0.0152\n",
            "Epoch 212 Batch 20 Loss 0.0169\n",
            "Epoch 212 Batch 30 Loss 0.0155\n",
            "Epoch 212 Loss 0.0123\n",
            "Time taken for 1 epoch 4.585948944091797 sec\n",
            "\n",
            "Epoch 213 Batch 0 Loss 0.0101\n",
            "Epoch 213 Batch 10 Loss 0.0245\n",
            "Epoch 213 Batch 20 Loss 0.0086\n",
            "Epoch 213 Batch 30 Loss 0.0133\n",
            "Epoch 213 Loss 0.0123\n",
            "Time taken for 1 epoch 4.433953523635864 sec\n",
            "\n",
            "Epoch 214 Batch 0 Loss 0.0059\n",
            "Epoch 214 Batch 10 Loss 0.0180\n",
            "Epoch 214 Batch 20 Loss 0.0113\n",
            "Epoch 214 Batch 30 Loss 0.0092\n",
            "Epoch 214 Loss 0.0126\n",
            "Time taken for 1 epoch 4.505300998687744 sec\n",
            "\n",
            "Epoch 215 Batch 0 Loss 0.0092\n",
            "Epoch 215 Batch 10 Loss 0.0166\n",
            "Epoch 215 Batch 20 Loss 0.0063\n",
            "Epoch 215 Batch 30 Loss 0.0135\n",
            "Epoch 215 Loss 0.0122\n",
            "Time taken for 1 epoch 4.349069118499756 sec\n",
            "\n",
            "Epoch 216 Batch 0 Loss 0.0073\n",
            "Epoch 216 Batch 10 Loss 0.0130\n",
            "Epoch 216 Batch 20 Loss 0.0292\n",
            "Epoch 216 Batch 30 Loss 0.0087\n",
            "Epoch 216 Loss 0.0132\n",
            "Time taken for 1 epoch 4.501925706863403 sec\n",
            "\n",
            "Epoch 217 Batch 0 Loss 0.0043\n",
            "Epoch 217 Batch 10 Loss 0.0105\n",
            "Epoch 217 Batch 20 Loss 0.0281\n",
            "Epoch 217 Batch 30 Loss 0.0185\n",
            "Epoch 217 Loss 0.0149\n",
            "Time taken for 1 epoch 4.387524843215942 sec\n",
            "\n",
            "Epoch 218 Batch 0 Loss 0.0158\n",
            "Epoch 218 Batch 10 Loss 0.0124\n",
            "Epoch 218 Batch 20 Loss 0.0134\n",
            "Epoch 218 Batch 30 Loss 0.0201\n",
            "Epoch 218 Loss 0.0207\n",
            "Time taken for 1 epoch 4.438849449157715 sec\n",
            "\n",
            "Epoch 219 Batch 0 Loss 0.0310\n",
            "Epoch 219 Batch 10 Loss 0.0167\n",
            "Epoch 219 Batch 20 Loss 0.0270\n",
            "Epoch 219 Batch 30 Loss 0.0441\n",
            "Epoch 219 Loss 0.0317\n",
            "Time taken for 1 epoch 4.313019037246704 sec\n",
            "\n",
            "Epoch 220 Batch 0 Loss 0.0391\n",
            "Epoch 220 Batch 10 Loss 0.0463\n",
            "Epoch 220 Batch 20 Loss 0.0418\n",
            "Epoch 220 Batch 30 Loss 0.0853\n",
            "Epoch 220 Loss 0.0534\n",
            "Time taken for 1 epoch 4.479769229888916 sec\n",
            "\n",
            "Epoch 221 Batch 0 Loss 0.0637\n",
            "Epoch 221 Batch 10 Loss 0.0752\n",
            "Epoch 221 Batch 20 Loss 0.1259\n",
            "Epoch 221 Batch 30 Loss 0.1158\n",
            "Epoch 221 Loss 0.0961\n",
            "Time taken for 1 epoch 4.3178136348724365 sec\n",
            "\n",
            "Epoch 222 Batch 0 Loss 0.1136\n",
            "Epoch 222 Batch 10 Loss 0.1150\n",
            "Epoch 222 Batch 20 Loss 0.1617\n",
            "Epoch 222 Batch 30 Loss 0.1462\n",
            "Epoch 222 Loss 0.1314\n",
            "Time taken for 1 epoch 4.45578408241272 sec\n",
            "\n",
            "Epoch 223 Batch 0 Loss 0.1411\n",
            "Epoch 223 Batch 10 Loss 0.1868\n",
            "Epoch 223 Batch 20 Loss 0.1917\n",
            "Epoch 223 Batch 30 Loss 0.1124\n",
            "Epoch 223 Loss 0.1571\n",
            "Time taken for 1 epoch 4.358248710632324 sec\n",
            "\n",
            "Epoch 224 Batch 0 Loss 0.1222\n",
            "Epoch 224 Batch 10 Loss 0.1596\n",
            "Epoch 224 Batch 20 Loss 0.1508\n",
            "Epoch 224 Batch 30 Loss 0.1053\n",
            "Epoch 224 Loss 0.1391\n",
            "Time taken for 1 epoch 4.45247220993042 sec\n",
            "\n",
            "Epoch 225 Batch 0 Loss 0.1276\n",
            "Epoch 225 Batch 10 Loss 0.1787\n",
            "Epoch 225 Batch 20 Loss 0.1437\n",
            "Epoch 225 Batch 30 Loss 0.0881\n",
            "Epoch 225 Loss 0.1155\n",
            "Time taken for 1 epoch 4.358131408691406 sec\n",
            "\n",
            "Epoch 226 Batch 0 Loss 0.0484\n",
            "Epoch 226 Batch 10 Loss 0.0847\n",
            "Epoch 226 Batch 20 Loss 0.0875\n",
            "Epoch 226 Batch 30 Loss 0.0589\n",
            "Epoch 226 Loss 0.0843\n",
            "Time taken for 1 epoch 4.456003427505493 sec\n",
            "\n",
            "Epoch 227 Batch 0 Loss 0.0462\n",
            "Epoch 227 Batch 10 Loss 0.0492\n",
            "Epoch 227 Batch 20 Loss 0.0653\n",
            "Epoch 227 Batch 30 Loss 0.1099\n",
            "Epoch 227 Loss 0.0630\n",
            "Time taken for 1 epoch 4.336876392364502 sec\n",
            "\n",
            "Epoch 228 Batch 0 Loss 0.0302\n",
            "Epoch 228 Batch 10 Loss 0.0571\n",
            "Epoch 228 Batch 20 Loss 0.0355\n",
            "Epoch 228 Batch 30 Loss 0.0588\n",
            "Epoch 228 Loss 0.0504\n",
            "Time taken for 1 epoch 4.456185817718506 sec\n",
            "\n",
            "Epoch 229 Batch 0 Loss 0.0350\n",
            "Epoch 229 Batch 10 Loss 0.0534\n",
            "Epoch 229 Batch 20 Loss 0.0451\n",
            "Epoch 229 Batch 30 Loss 0.0587\n",
            "Epoch 229 Loss 0.0390\n",
            "Time taken for 1 epoch 4.523627042770386 sec\n",
            "\n",
            "Epoch 230 Batch 0 Loss 0.0437\n",
            "Epoch 230 Batch 10 Loss 0.0276\n",
            "Epoch 230 Batch 20 Loss 0.0265\n",
            "Epoch 230 Batch 30 Loss 0.0326\n",
            "Epoch 230 Loss 0.0314\n",
            "Time taken for 1 epoch 4.430983066558838 sec\n",
            "\n",
            "Epoch 231 Batch 0 Loss 0.0168\n",
            "Epoch 231 Batch 10 Loss 0.0131\n",
            "Epoch 231 Batch 20 Loss 0.0179\n",
            "Epoch 231 Batch 30 Loss 0.0133\n",
            "Epoch 231 Loss 0.0251\n",
            "Time taken for 1 epoch 4.3385233879089355 sec\n",
            "\n",
            "Epoch 232 Batch 0 Loss 0.0163\n",
            "Epoch 232 Batch 10 Loss 0.0215\n",
            "Epoch 232 Batch 20 Loss 0.0209\n",
            "Epoch 232 Batch 30 Loss 0.0226\n",
            "Epoch 232 Loss 0.0221\n",
            "Time taken for 1 epoch 4.477544069290161 sec\n",
            "\n",
            "Epoch 233 Batch 0 Loss 0.0224\n",
            "Epoch 233 Batch 10 Loss 0.0150\n",
            "Epoch 233 Batch 20 Loss 0.0097\n",
            "Epoch 233 Batch 30 Loss 0.0236\n",
            "Epoch 233 Loss 0.0194\n",
            "Time taken for 1 epoch 4.373678684234619 sec\n",
            "\n",
            "Epoch 234 Batch 0 Loss 0.0303\n",
            "Epoch 234 Batch 10 Loss 0.0204\n",
            "Epoch 234 Batch 20 Loss 0.0102\n",
            "Epoch 234 Batch 30 Loss 0.0121\n",
            "Epoch 234 Loss 0.0182\n",
            "Time taken for 1 epoch 4.492435455322266 sec\n",
            "\n",
            "Epoch 235 Batch 0 Loss 0.0126\n",
            "Epoch 235 Batch 10 Loss 0.0242\n",
            "Epoch 235 Batch 20 Loss 0.0134\n",
            "Epoch 235 Batch 30 Loss 0.0110\n",
            "Epoch 235 Loss 0.0167\n",
            "Time taken for 1 epoch 4.3651556968688965 sec\n",
            "\n",
            "Epoch 236 Batch 0 Loss 0.0201\n",
            "Epoch 236 Batch 10 Loss 0.0170\n",
            "Epoch 236 Batch 20 Loss 0.0240\n",
            "Epoch 236 Batch 30 Loss 0.0079\n",
            "Epoch 236 Loss 0.0172\n",
            "Time taken for 1 epoch 4.643051385879517 sec\n",
            "\n",
            "Epoch 237 Batch 0 Loss 0.0075\n",
            "Epoch 237 Batch 10 Loss 0.0262\n",
            "Epoch 237 Batch 20 Loss 0.0265\n",
            "Epoch 237 Batch 30 Loss 0.0110\n",
            "Epoch 237 Loss 0.0146\n",
            "Time taken for 1 epoch 4.414126634597778 sec\n",
            "\n",
            "Epoch 238 Batch 0 Loss 0.0163\n",
            "Epoch 238 Batch 10 Loss 0.0166\n",
            "Epoch 238 Batch 20 Loss 0.0091\n",
            "Epoch 238 Batch 30 Loss 0.0132\n",
            "Epoch 238 Loss 0.0146\n",
            "Time taken for 1 epoch 4.510851144790649 sec\n",
            "\n",
            "Epoch 239 Batch 0 Loss 0.0071\n",
            "Epoch 239 Batch 10 Loss 0.0085\n",
            "Epoch 239 Batch 20 Loss 0.0133\n",
            "Epoch 239 Batch 30 Loss 0.0101\n",
            "Epoch 239 Loss 0.0148\n",
            "Time taken for 1 epoch 4.340862035751343 sec\n",
            "\n",
            "Epoch 240 Batch 0 Loss 0.0150\n",
            "Epoch 240 Batch 10 Loss 0.0088\n",
            "Epoch 240 Batch 20 Loss 0.0105\n",
            "Epoch 240 Batch 30 Loss 0.0167\n",
            "Epoch 240 Loss 0.0140\n",
            "Time taken for 1 epoch 4.426758289337158 sec\n",
            "\n",
            "Epoch 241 Batch 0 Loss 0.0153\n",
            "Epoch 241 Batch 10 Loss 0.0174\n",
            "Epoch 241 Batch 20 Loss 0.0085\n",
            "Epoch 241 Batch 30 Loss 0.0043\n",
            "Epoch 241 Loss 0.0139\n",
            "Time taken for 1 epoch 4.302903890609741 sec\n",
            "\n",
            "Epoch 242 Batch 0 Loss 0.0150\n",
            "Epoch 242 Batch 10 Loss 0.0113\n",
            "Epoch 242 Batch 20 Loss 0.0042\n",
            "Epoch 242 Batch 30 Loss 0.0117\n",
            "Epoch 242 Loss 0.0130\n",
            "Time taken for 1 epoch 4.435080528259277 sec\n",
            "\n",
            "Epoch 243 Batch 0 Loss 0.0097\n",
            "Epoch 243 Batch 10 Loss 0.0126\n",
            "Epoch 243 Batch 20 Loss 0.0118\n",
            "Epoch 243 Batch 30 Loss 0.0234\n",
            "Epoch 243 Loss 0.0124\n",
            "Time taken for 1 epoch 4.3595311641693115 sec\n",
            "\n",
            "Epoch 244 Batch 0 Loss 0.0096\n",
            "Epoch 244 Batch 10 Loss 0.0151\n",
            "Epoch 244 Batch 20 Loss 0.0056\n",
            "Epoch 244 Batch 30 Loss 0.0157\n",
            "Epoch 244 Loss 0.0115\n",
            "Time taken for 1 epoch 4.499476671218872 sec\n",
            "\n",
            "Epoch 245 Batch 0 Loss 0.0081\n",
            "Epoch 245 Batch 10 Loss 0.0084\n",
            "Epoch 245 Batch 20 Loss 0.0134\n",
            "Epoch 245 Batch 30 Loss 0.0078\n",
            "Epoch 245 Loss 0.0119\n",
            "Time taken for 1 epoch 4.345012664794922 sec\n",
            "\n",
            "Epoch 246 Batch 0 Loss 0.0167\n",
            "Epoch 246 Batch 10 Loss 0.0178\n",
            "Epoch 246 Batch 20 Loss 0.0168\n",
            "Epoch 246 Batch 30 Loss 0.0097\n",
            "Epoch 246 Loss 0.0120\n",
            "Time taken for 1 epoch 4.43168044090271 sec\n",
            "\n",
            "Epoch 247 Batch 0 Loss 0.0134\n",
            "Epoch 247 Batch 10 Loss 0.0104\n",
            "Epoch 247 Batch 20 Loss 0.0092\n",
            "Epoch 247 Batch 30 Loss 0.0045\n",
            "Epoch 247 Loss 0.0110\n",
            "Time taken for 1 epoch 4.322719097137451 sec\n",
            "\n",
            "Epoch 248 Batch 0 Loss 0.0198\n",
            "Epoch 248 Batch 10 Loss 0.0133\n",
            "Epoch 248 Batch 20 Loss 0.0095\n",
            "Epoch 248 Batch 30 Loss 0.0114\n",
            "Epoch 248 Loss 0.0124\n",
            "Time taken for 1 epoch 4.4728477001190186 sec\n",
            "\n",
            "Epoch 249 Batch 0 Loss 0.0169\n",
            "Epoch 249 Batch 10 Loss 0.0060\n",
            "Epoch 249 Batch 20 Loss 0.0155\n",
            "Epoch 249 Batch 30 Loss 0.0090\n",
            "Epoch 249 Loss 0.0122\n",
            "Time taken for 1 epoch 4.332832336425781 sec\n",
            "\n",
            "Epoch 250 Batch 0 Loss 0.0061\n",
            "Epoch 250 Batch 10 Loss 0.0052\n",
            "Epoch 250 Batch 20 Loss 0.0077\n",
            "Epoch 250 Batch 30 Loss 0.0163\n",
            "Epoch 250 Loss 0.0126\n",
            "Time taken for 1 epoch 4.438792705535889 sec\n",
            "\n",
            "Epoch 251 Batch 0 Loss 0.0139\n",
            "Epoch 251 Batch 10 Loss 0.0153\n",
            "Epoch 251 Batch 20 Loss 0.0187\n",
            "Epoch 251 Batch 30 Loss 0.0036\n",
            "Epoch 251 Loss 0.0119\n",
            "Time taken for 1 epoch 4.329374074935913 sec\n",
            "\n",
            "Epoch 252 Batch 0 Loss 0.0126\n",
            "Epoch 252 Batch 10 Loss 0.0052\n",
            "Epoch 252 Batch 20 Loss 0.0055\n",
            "Epoch 252 Batch 30 Loss 0.0189\n",
            "Epoch 252 Loss 0.0123\n",
            "Time taken for 1 epoch 4.484996795654297 sec\n",
            "\n",
            "Epoch 253 Batch 0 Loss 0.0219\n",
            "Epoch 253 Batch 10 Loss 0.0067\n",
            "Epoch 253 Batch 20 Loss 0.0132\n",
            "Epoch 253 Batch 30 Loss 0.0127\n",
            "Epoch 253 Loss 0.0122\n",
            "Time taken for 1 epoch 4.321459531784058 sec\n",
            "\n",
            "Epoch 254 Batch 0 Loss 0.0117\n",
            "Epoch 254 Batch 10 Loss 0.0148\n",
            "Epoch 254 Batch 20 Loss 0.0282\n",
            "Epoch 254 Batch 30 Loss 0.0051\n",
            "Epoch 254 Loss 0.0121\n",
            "Time taken for 1 epoch 4.428244590759277 sec\n",
            "\n",
            "Epoch 255 Batch 0 Loss 0.0204\n",
            "Epoch 255 Batch 10 Loss 0.0196\n",
            "Epoch 255 Batch 20 Loss 0.0131\n",
            "Epoch 255 Batch 30 Loss 0.0191\n",
            "Epoch 255 Loss 0.0125\n",
            "Time taken for 1 epoch 4.319328546524048 sec\n",
            "\n",
            "Epoch 256 Batch 0 Loss 0.0081\n",
            "Epoch 256 Batch 10 Loss 0.0021\n",
            "Epoch 256 Batch 20 Loss 0.0140\n",
            "Epoch 256 Batch 30 Loss 0.0181\n",
            "Epoch 256 Loss 0.0117\n",
            "Time taken for 1 epoch 4.548298358917236 sec\n",
            "\n",
            "Epoch 257 Batch 0 Loss 0.0132\n",
            "Epoch 257 Batch 10 Loss 0.0088\n",
            "Epoch 257 Batch 20 Loss 0.0133\n",
            "Epoch 257 Batch 30 Loss 0.0051\n",
            "Epoch 257 Loss 0.0118\n",
            "Time taken for 1 epoch 4.398651123046875 sec\n",
            "\n",
            "Epoch 258 Batch 0 Loss 0.0090\n",
            "Epoch 258 Batch 10 Loss 0.0176\n",
            "Epoch 258 Batch 20 Loss 0.0215\n",
            "Epoch 258 Batch 30 Loss 0.0139\n",
            "Epoch 258 Loss 0.0111\n",
            "Time taken for 1 epoch 4.509230375289917 sec\n",
            "\n",
            "Epoch 259 Batch 0 Loss 0.0138\n",
            "Epoch 259 Batch 10 Loss 0.0153\n",
            "Epoch 259 Batch 20 Loss 0.0108\n",
            "Epoch 259 Batch 30 Loss 0.0078\n",
            "Epoch 259 Loss 0.0118\n",
            "Time taken for 1 epoch 4.402038097381592 sec\n",
            "\n",
            "Epoch 260 Batch 0 Loss 0.0137\n",
            "Epoch 260 Batch 10 Loss 0.0174\n",
            "Epoch 260 Batch 20 Loss 0.0197\n",
            "Epoch 260 Batch 30 Loss 0.0188\n",
            "Epoch 260 Loss 0.0120\n",
            "Time taken for 1 epoch 4.546816825866699 sec\n",
            "\n",
            "Epoch 261 Batch 0 Loss 0.0179\n",
            "Epoch 261 Batch 10 Loss 0.0060\n",
            "Epoch 261 Batch 20 Loss 0.0050\n",
            "Epoch 261 Batch 30 Loss 0.0168\n",
            "Epoch 261 Loss 0.0120\n",
            "Time taken for 1 epoch 4.439291954040527 sec\n",
            "\n",
            "Epoch 262 Batch 0 Loss 0.0081\n",
            "Epoch 262 Batch 10 Loss 0.0185\n",
            "Epoch 262 Batch 20 Loss 0.0090\n",
            "Epoch 262 Batch 30 Loss 0.0518\n",
            "Epoch 262 Loss 0.0134\n",
            "Time taken for 1 epoch 4.485773086547852 sec\n",
            "\n",
            "Epoch 263 Batch 0 Loss 0.0145\n",
            "Epoch 263 Batch 10 Loss 0.0016\n",
            "Epoch 263 Batch 20 Loss 0.0071\n",
            "Epoch 263 Batch 30 Loss 0.0166\n",
            "Epoch 263 Loss 0.0125\n",
            "Time taken for 1 epoch 4.322116851806641 sec\n",
            "\n",
            "Epoch 264 Batch 0 Loss 0.0068\n",
            "Epoch 264 Batch 10 Loss 0.0161\n",
            "Epoch 264 Batch 20 Loss 0.0296\n",
            "Epoch 264 Batch 30 Loss 0.0224\n",
            "Epoch 264 Loss 0.0136\n",
            "Time taken for 1 epoch 4.535109758377075 sec\n",
            "\n",
            "Epoch 265 Batch 0 Loss 0.0093\n",
            "Epoch 265 Batch 10 Loss 0.0169\n",
            "Epoch 265 Batch 20 Loss 0.0149\n",
            "Epoch 265 Batch 30 Loss 0.0101\n",
            "Epoch 265 Loss 0.0125\n",
            "Time taken for 1 epoch 4.35860538482666 sec\n",
            "\n",
            "Epoch 266 Batch 0 Loss 0.0113\n",
            "Epoch 266 Batch 10 Loss 0.0047\n",
            "Epoch 266 Batch 20 Loss 0.0147\n",
            "Epoch 266 Batch 30 Loss 0.0063\n",
            "Epoch 266 Loss 0.0136\n",
            "Time taken for 1 epoch 4.463201999664307 sec\n",
            "\n",
            "Epoch 267 Batch 0 Loss 0.0108\n",
            "Epoch 267 Batch 10 Loss 0.0084\n",
            "Epoch 267 Batch 20 Loss 0.0265\n",
            "Epoch 267 Batch 30 Loss 0.0166\n",
            "Epoch 267 Loss 0.0139\n",
            "Time taken for 1 epoch 4.31652569770813 sec\n",
            "\n",
            "Epoch 268 Batch 0 Loss 0.0089\n",
            "Epoch 268 Batch 10 Loss 0.0101\n",
            "Epoch 268 Batch 20 Loss 0.0306\n",
            "Epoch 268 Batch 30 Loss 0.0161\n",
            "Epoch 268 Loss 0.0154\n",
            "Time taken for 1 epoch 4.473764657974243 sec\n",
            "\n",
            "Epoch 269 Batch 0 Loss 0.0130\n",
            "Epoch 269 Batch 10 Loss 0.0279\n",
            "Epoch 269 Batch 20 Loss 0.0156\n",
            "Epoch 269 Batch 30 Loss 0.0142\n",
            "Epoch 269 Loss 0.0154\n",
            "Time taken for 1 epoch 4.354535341262817 sec\n",
            "\n",
            "Epoch 270 Batch 0 Loss 0.0252\n",
            "Epoch 270 Batch 10 Loss 0.0102\n",
            "Epoch 270 Batch 20 Loss 0.0468\n",
            "Epoch 270 Batch 30 Loss 0.0092\n",
            "Epoch 270 Loss 0.0198\n",
            "Time taken for 1 epoch 4.465495824813843 sec\n",
            "\n",
            "Epoch 271 Batch 0 Loss 0.0118\n",
            "Epoch 271 Batch 10 Loss 0.0112\n",
            "Epoch 271 Batch 20 Loss 0.0382\n",
            "Epoch 271 Batch 30 Loss 0.0118\n",
            "Epoch 271 Loss 0.0229\n",
            "Time taken for 1 epoch 4.321260690689087 sec\n",
            "\n",
            "Epoch 272 Batch 0 Loss 0.0088\n",
            "Epoch 272 Batch 10 Loss 0.0163\n",
            "Epoch 272 Batch 20 Loss 0.0217\n",
            "Epoch 272 Batch 30 Loss 0.0366\n",
            "Epoch 272 Loss 0.0292\n",
            "Time taken for 1 epoch 4.55609655380249 sec\n",
            "\n",
            "Epoch 273 Batch 0 Loss 0.0186\n",
            "Epoch 273 Batch 10 Loss 0.0184\n",
            "Epoch 273 Batch 20 Loss 0.0347\n",
            "Epoch 273 Batch 30 Loss 0.0636\n",
            "Epoch 273 Loss 0.0482\n",
            "Time taken for 1 epoch 4.322406053543091 sec\n",
            "\n",
            "Epoch 274 Batch 0 Loss 0.0281\n",
            "Epoch 274 Batch 10 Loss 0.0482\n",
            "Epoch 274 Batch 20 Loss 0.0626\n",
            "Epoch 274 Batch 30 Loss 0.0693\n",
            "Epoch 274 Loss 0.0714\n",
            "Time taken for 1 epoch 4.426136493682861 sec\n",
            "\n",
            "Epoch 275 Batch 0 Loss 0.0422\n",
            "Epoch 275 Batch 10 Loss 0.0904\n",
            "Epoch 275 Batch 20 Loss 0.0969\n",
            "Epoch 275 Batch 30 Loss 0.1003\n",
            "Epoch 275 Loss 0.0944\n",
            "Time taken for 1 epoch 4.326089859008789 sec\n",
            "\n",
            "Epoch 276 Batch 0 Loss 0.1149\n",
            "Epoch 276 Batch 10 Loss 0.1339\n",
            "Epoch 276 Batch 20 Loss 0.1568\n",
            "Epoch 276 Batch 30 Loss 0.0945\n",
            "Epoch 276 Loss 0.1215\n",
            "Time taken for 1 epoch 4.496382713317871 sec\n",
            "\n",
            "Epoch 277 Batch 0 Loss 0.1326\n",
            "Epoch 277 Batch 10 Loss 0.0878\n",
            "Epoch 277 Batch 20 Loss 0.1444\n",
            "Epoch 277 Batch 30 Loss 0.1105\n",
            "Epoch 277 Loss 0.1324\n",
            "Time taken for 1 epoch 4.309193849563599 sec\n",
            "\n",
            "Epoch 278 Batch 0 Loss 0.0986\n",
            "Epoch 278 Batch 10 Loss 0.1239\n",
            "Epoch 278 Batch 20 Loss 0.0995\n",
            "Epoch 278 Batch 30 Loss 0.0973\n",
            "Epoch 278 Loss 0.1002\n",
            "Time taken for 1 epoch 4.472708702087402 sec\n",
            "\n",
            "Epoch 279 Batch 0 Loss 0.0712\n",
            "Epoch 279 Batch 10 Loss 0.0610\n",
            "Epoch 279 Batch 20 Loss 0.0630\n",
            "Epoch 279 Batch 30 Loss 0.0515\n",
            "Epoch 279 Loss 0.0710\n",
            "Time taken for 1 epoch 4.341246604919434 sec\n",
            "\n",
            "Epoch 280 Batch 0 Loss 0.0485\n",
            "Epoch 280 Batch 10 Loss 0.0333\n",
            "Epoch 280 Batch 20 Loss 0.0707\n",
            "Epoch 280 Batch 30 Loss 0.0990\n",
            "Epoch 280 Loss 0.0559\n",
            "Time taken for 1 epoch 4.548295974731445 sec\n",
            "\n",
            "Epoch 281 Batch 0 Loss 0.0344\n",
            "Epoch 281 Batch 10 Loss 0.0475\n",
            "Epoch 281 Batch 20 Loss 0.0458\n",
            "Epoch 281 Batch 30 Loss 0.0406\n",
            "Epoch 281 Loss 0.0377\n",
            "Time taken for 1 epoch 4.380924463272095 sec\n",
            "\n",
            "Epoch 282 Batch 0 Loss 0.0272\n",
            "Epoch 282 Batch 10 Loss 0.0188\n",
            "Epoch 282 Batch 20 Loss 0.0205\n",
            "Epoch 282 Batch 30 Loss 0.0501\n",
            "Epoch 282 Loss 0.0308\n",
            "Time taken for 1 epoch 4.505671739578247 sec\n",
            "\n",
            "Epoch 283 Batch 0 Loss 0.0281\n",
            "Epoch 283 Batch 10 Loss 0.0505\n",
            "Epoch 283 Batch 20 Loss 0.0333\n",
            "Epoch 283 Batch 30 Loss 0.0174\n",
            "Epoch 283 Loss 0.0258\n",
            "Time taken for 1 epoch 4.420959949493408 sec\n",
            "\n",
            "Epoch 284 Batch 0 Loss 0.0406\n",
            "Epoch 284 Batch 10 Loss 0.0380\n",
            "Epoch 284 Batch 20 Loss 0.0149\n",
            "Epoch 284 Batch 30 Loss 0.0365\n",
            "Epoch 284 Loss 0.0226\n",
            "Time taken for 1 epoch 4.525906562805176 sec\n",
            "\n",
            "Epoch 285 Batch 0 Loss 0.0183\n",
            "Epoch 285 Batch 10 Loss 0.0144\n",
            "Epoch 285 Batch 20 Loss 0.0105\n",
            "Epoch 285 Batch 30 Loss 0.0126\n",
            "Epoch 285 Loss 0.0200\n",
            "Time taken for 1 epoch 4.372913837432861 sec\n",
            "\n",
            "Epoch 286 Batch 0 Loss 0.0082\n",
            "Epoch 286 Batch 10 Loss 0.0267\n",
            "Epoch 286 Batch 20 Loss 0.0134\n",
            "Epoch 286 Batch 30 Loss 0.0164\n",
            "Epoch 286 Loss 0.0192\n",
            "Time taken for 1 epoch 4.493950605392456 sec\n",
            "\n",
            "Epoch 287 Batch 0 Loss 0.0118\n",
            "Epoch 287 Batch 10 Loss 0.0105\n",
            "Epoch 287 Batch 20 Loss 0.0098\n",
            "Epoch 287 Batch 30 Loss 0.0064\n",
            "Epoch 287 Loss 0.0142\n",
            "Time taken for 1 epoch 4.178982496261597 sec\n",
            "\n",
            "Epoch 288 Batch 0 Loss 0.0100\n",
            "Epoch 288 Batch 10 Loss 0.0105\n",
            "Epoch 288 Batch 20 Loss 0.0222\n",
            "Epoch 288 Batch 30 Loss 0.0122\n",
            "Epoch 288 Loss 0.0143\n",
            "Time taken for 1 epoch 4.280252456665039 sec\n",
            "\n",
            "Epoch 289 Batch 0 Loss 0.0062\n",
            "Epoch 289 Batch 10 Loss 0.0218\n",
            "Epoch 289 Batch 20 Loss 0.0089\n",
            "Epoch 289 Batch 30 Loss 0.0076\n",
            "Epoch 289 Loss 0.0134\n",
            "Time taken for 1 epoch 4.1153950691223145 sec\n",
            "\n",
            "Epoch 290 Batch 0 Loss 0.0103\n",
            "Epoch 290 Batch 10 Loss 0.0154\n",
            "Epoch 290 Batch 20 Loss 0.0217\n",
            "Epoch 290 Batch 30 Loss 0.0182\n",
            "Epoch 290 Loss 0.0133\n",
            "Time taken for 1 epoch 4.341625690460205 sec\n",
            "\n",
            "Epoch 291 Batch 0 Loss 0.0155\n",
            "Epoch 291 Batch 10 Loss 0.0037\n",
            "Epoch 291 Batch 20 Loss 0.0214\n",
            "Epoch 291 Batch 30 Loss 0.0119\n",
            "Epoch 291 Loss 0.0118\n",
            "Time taken for 1 epoch 4.210252046585083 sec\n",
            "\n",
            "Epoch 292 Batch 0 Loss 0.0079\n",
            "Epoch 292 Batch 10 Loss 0.0060\n",
            "Epoch 292 Batch 20 Loss 0.0069\n",
            "Epoch 292 Batch 30 Loss 0.0106\n",
            "Epoch 292 Loss 0.0121\n",
            "Time taken for 1 epoch 4.262152433395386 sec\n",
            "\n",
            "Epoch 293 Batch 0 Loss 0.0170\n",
            "Epoch 293 Batch 10 Loss 0.0125\n",
            "Epoch 293 Batch 20 Loss 0.0175\n",
            "Epoch 293 Batch 30 Loss 0.0058\n",
            "Epoch 293 Loss 0.0119\n",
            "Time taken for 1 epoch 4.198901891708374 sec\n",
            "\n",
            "Epoch 294 Batch 0 Loss 0.0115\n",
            "Epoch 294 Batch 10 Loss 0.0056\n",
            "Epoch 294 Batch 20 Loss 0.0081\n",
            "Epoch 294 Batch 30 Loss 0.0129\n",
            "Epoch 294 Loss 0.0115\n",
            "Time taken for 1 epoch 4.384710073471069 sec\n",
            "\n",
            "Epoch 295 Batch 0 Loss 0.0090\n",
            "Epoch 295 Batch 10 Loss 0.0091\n",
            "Epoch 295 Batch 20 Loss 0.0147\n",
            "Epoch 295 Batch 30 Loss 0.0055\n",
            "Epoch 295 Loss 0.0109\n",
            "Time taken for 1 epoch 4.238642692565918 sec\n",
            "\n",
            "Epoch 296 Batch 0 Loss 0.0162\n",
            "Epoch 296 Batch 10 Loss 0.0085\n",
            "Epoch 296 Batch 20 Loss 0.0174\n",
            "Epoch 296 Batch 30 Loss 0.0091\n",
            "Epoch 296 Loss 0.0105\n",
            "Time taken for 1 epoch 4.330213308334351 sec\n",
            "\n",
            "Epoch 297 Batch 0 Loss 0.0121\n",
            "Epoch 297 Batch 10 Loss 0.0074\n",
            "Epoch 297 Batch 20 Loss 0.0063\n",
            "Epoch 297 Batch 30 Loss 0.0137\n",
            "Epoch 297 Loss 0.0111\n",
            "Time taken for 1 epoch 4.281530141830444 sec\n",
            "\n",
            "Epoch 298 Batch 0 Loss 0.0103\n",
            "Epoch 298 Batch 10 Loss 0.0076\n",
            "Epoch 298 Batch 20 Loss 0.0032\n",
            "Epoch 298 Batch 30 Loss 0.0031\n",
            "Epoch 298 Loss 0.0110\n",
            "Time taken for 1 epoch 4.367893218994141 sec\n",
            "\n",
            "Epoch 299 Batch 0 Loss 0.0217\n",
            "Epoch 299 Batch 10 Loss 0.0212\n",
            "Epoch 299 Batch 20 Loss 0.0128\n",
            "Epoch 299 Batch 30 Loss 0.0177\n",
            "Epoch 299 Loss 0.0109\n",
            "Time taken for 1 epoch 4.256072998046875 sec\n",
            "\n",
            "Epoch 300 Batch 0 Loss 0.0033\n",
            "Epoch 300 Batch 10 Loss 0.0120\n",
            "Epoch 300 Batch 20 Loss 0.0157\n",
            "Epoch 300 Batch 30 Loss 0.0067\n",
            "Epoch 300 Loss 0.0113\n",
            "Time taken for 1 epoch 4.398711919784546 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-_2RdeiKAAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4617e87f-1554-4734-cc58-34382c122a66"
      },
      "source": [
        "for example, label in test_data.take(3):\n",
        "# initialisation sur un exemple du test\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  input_t = example[0]\n",
        "  output_label = label[0]\n",
        "  enc_out, enc_hidden = encoder(tf.expand_dims(input_t, axis=0), hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([all_tokens[\"<start>\"]], 0)\n",
        "\n",
        "  result = \"\"\n",
        "\n",
        "\n",
        "# Model lyrics\n",
        "  for t in range(padded_text_indices.shape[-1]):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                          dec_hidden,\n",
        "                                                          enc_out)\n",
        "\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    corresponding_word = [word for word, id in all_tokens.items() if id==predicted_id]\n",
        "    result += corresponding_word[0] + \" \"\n",
        "\n",
        "    if corresponding_word[0] == '<end>':\n",
        "      break\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "# Text sentence\n",
        "  input_sentence = \"\"\n",
        "  for token_id in input_t:\n",
        "    if token_id==0:\n",
        "      break\n",
        "    \n",
        "    corresponding_word = [word for word, id in all_tokens.items() if id==token_id]\n",
        "    input_sentence += corresponding_word[0] + \" \"\n",
        "    if corresponding_word[0] == \"<end>\":\n",
        "      break\n",
        "\n",
        "# True lyrics\n",
        "  true_translation = \"\"\n",
        "\n",
        "  for token_id in output_label:\n",
        "    if token_id==0:\n",
        "      break\n",
        "    corresponding_word = [word for word, id in all_tokens.items() if id==token_id]\n",
        "    true_translation += corresponding_word[0] + \" \"\n",
        "    if corresponding_word[0] == \"<end>\":\n",
        "      break \n",
        "\n",
        "\n",
        "print(\"text sentence: {}\".format(input_sentence))\n",
        "print(\"True lyrics: {}\".format(true_translation))\n",
        "print(\"Model lyrics: {}\".format(result))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text sentence: l' air inspiré entre par le nez descend par la trachée <lb> les bronches les bronchioles jusqu' aux alvéoles pulmonaires <lb> cet air contient tous les gaz présents dans l' atmosphère \n",
            "True lyrics: <start> l' air qui est inspiré entre par le nez ensuite il descend par la trachée puis par les bronches les bronchioles jusqu' aux alvéoles pulmonaires cet air contient tous les gaz présents dans l' atmosphère <end> \n",
            "Model lyrics: l' air qui est inspiré entre par le nez ensuite il descend par la trachée puis par les bronches les bronchioles jusqu' aux alvéoles pulmonaires cet air contient tous les gaz présents dans l' atmosphère <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avc9XnzvX_5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3-vvM-uYIz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyiEr6dSYNTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}